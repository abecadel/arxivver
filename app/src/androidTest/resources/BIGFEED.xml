<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <link href="http://arxiv.org/api/query?search_query%3DLSTM%26id_list%3D%26start%3D0%26max_results%3D100" rel="self" type="application/atom+xml"/>
    <title type="html">ArXiv Query: search_query=LSTM&amp;id_list=&amp;start=0&amp;max_results=100</title>
    <id>http://arxiv.org/api/7EGwT3Jtvn8xPpwfq0nyfPhGuRc</id>
    <updated>2017-11-01T00:00:00-04:00</updated>
    <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">796</opensearch:totalResults>
    <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
    <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">100</opensearch:itemsPerPage>
    <entry>
        <id>http://arxiv.org/abs/1508.01991v1</id>
        <updated>2015-08-09T06:32:47Z</updated>
        <published>2015-08-09T06:32:47Z</published>
        <title>Bidirectional LSTM-CRF Models for Sequence Tagging</title>
        <summary>  In this paper, we propose a variety of Long Short-Term Memory (LSTM) based
            models for sequence tagging. These models include LSTM networks, bidirectional
            LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer
            (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is
            the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to
            NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model
            can efficiently use both past and future input features thanks to a
            bidirectional LSTM component. It can also use sentence level tag information
            thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or
            close to) accuracy on POS, chunking and NER data sets. In addition, it is
            robust and has less dependence on word embedding as compared to previous
            observations.
        </summary>
        <author>
            <name>Zhiheng Huang</name>
        </author>
        <author>
            <name>Wei Xu</name>
        </author>
        <author>
            <name>Kai Yu</name>
        </author>
        <link href="http://arxiv.org/abs/1508.01991v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1508.01991v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1709.06436v1</id>
        <updated>2017-09-19T14:04:15Z</updated>
        <published>2017-09-19T14:04:15Z</published>
        <title>Language Modeling with Highway LSTM</title>
        <summary>  Language models (LMs) based on Long Short Term Memory (LSTM) have shown good
            gains in many automatic speech recognition tasks. In this paper, we extend an
            LSTM by adding highway networks inside an LSTM and use the resulting Highway
            LSTM (HW-LSTM) model for language modeling. The added highway networks increase
            the depth in the time dimension. Since a typical LSTM has two internal states,
            a memory cell and a hidden state, we compare various types of HW-LSTM by adding
            highway networks onto the memory cell and/or the hidden state. Experimental
            results on English broadcast news and conversational telephone speech
            recognition show that the proposed HW-LSTM LM improves speech recognition
            accuracy on top of a strong LSTM LM baseline. We report 5.1% and 9.9% on the
            Switchboard and CallHome subsets of the Hub5 2000 evaluation, which reaches the
            best performance numbers reported on these tasks to date.
        </summary>
        <author>
            <name>Gakuto Kurata</name>
        </author>
        <author>
            <name>Bhuvana Ramabhadran</name>
        </author>
        <author>
            <name>George Saon</name>
        </author>
        <author>
            <name>Abhinav Sethy</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in 2017 IEEE Automatic Speech Recognition and Understanding
            Workshop (ASRU 2017)</arxiv:comment>
        <link href="http://arxiv.org/abs/1709.06436v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1709.06436v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1710.10197v1</id>
        <updated>2017-10-27T15:29:09Z</updated>
        <published>2017-10-27T15:29:09Z</published>
        <title>Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion
            Recognition</title>
        <summary>  Long short-term memory (LSTM) is normally used in recurrent neural network
            (RNN) as basic recurrent unit. However,conventional LSTM assumes that the state
            at current time step depends on previous time step. This assumption constraints
            the time dependency modeling capability. In this study, we propose a new
            variation of LSTM, advanced LSTM (A-LSTM), for better temporal context
            modeling. We employ A-LSTM in weighted pooling RNN for emotion recognition. The
            A-LSTM outperforms the conventional LSTM by 5.5% relatively. The A-LSTM based
            weighted pooling RNN can also complement the state-of-the-art emotion
            classification framework. This shows the advantage of A-LSTM.
        </summary>
        <author>
            <name>Fei Tao</name>
        </author>
        <author>
            <name>Gang Liu</name>
        </author>
        <link href="http://arxiv.org/abs/1710.10197v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1710.10197v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
        <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1701.03360v3</id>
        <updated>2017-06-05T18:51:08Z</updated>
        <published>2017-01-10T20:03:37Z</published>
        <title>Residual LSTM: Design of a Deep Recurrent Architecture for Distant
            Speech Recognition</title>
        <summary>  In this paper, a novel architecture for a deep recurrent neural network,
            residual LSTM is introduced. A plain LSTM has an internal memory cell that can
            learn long term dependencies of sequential data. It also provides a temporal
            shortcut path to avoid vanishing or exploding gradients in the temporal domain.
            The residual LSTM provides an additional spatial shortcut path from lower
            layers for efficient training of deep networks with multiple LSTM layers.
            Compared with the previous work, highway LSTM, residual LSTM separates a
            spatial shortcut path with temporal one by using output layers, which can help
            to avoid a conflict between spatial and temporal-domain gradient flows.
            Furthermore, residual LSTM reuses the output projection matrix and the output
            gate of LSTM to control the spatial information flow instead of additional gate
            networks, which effectively reduces more than 10% of network parameters. An
            experiment for distant speech recognition on the AMI SDM corpus shows that
            10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in
            WER over 3-layer aselines, respectively. On the contrary, 10-layer residual
            LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8%
            WER reduction over plain and highway LSTM networks, respectively.
        </summary>
        <author>
            <name>Jaeyoung Kim</name>
        </author>
        <author>
            <name>Mostafa El-Khamy</name>
        </author>
        <author>
            <name>Jungwon Lee</name>
        </author>
        <link href="http://arxiv.org/abs/1701.03360v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1701.03360v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1703.03055v1</id>
        <updated>2017-03-08T22:09:38Z</updated>
        <published>2017-03-08T22:09:38Z</published>
        <title>Interpretable Structure-Evolving LSTM</title>
        <summary>  This paper develops a general framework for learning interpretable data
            representation via Long Short-Term Memory (LSTM) recurrent neural networks over
            hierarchal graph structures. Instead of learning LSTM models over the pre-fixed
            structures, we propose to further learn the intermediate interpretable
            multi-level graph structures in a progressive and stochastic way from data
            during the LSTM network optimization. We thus call this model the
            structure-evolving LSTM. In particular, starting with an initial element-level
            graph representation where each node is a small data element, the
            structure-evolving LSTM gradually evolves the multi-level graph representations
            by stochastically merging the graph nodes with high compatibilities along the
            stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two
            connected nodes from their corresponding LSTM gate outputs, which is used to
            generate a merging probability. The candidate graph structures are accordingly
            generated where the nodes are grouped into cliques with their merging
            probabilities. We then produce the new graph structure with a
            Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in
            local optimums by stochastic sampling with an acceptance probability. Once a
            graph structure is accepted, a higher-level graph is then constructed by taking
            the partitioned cliques as its nodes. During the evolving process,
            representation becomes more abstracted in higher-levels where redundant
            information is filtered out, allowing more efficient propagation of long-range
            data dependencies. We evaluate the effectiveness of structure-evolving LSTM in
            the application of semantic object parsing and demonstrate its advantage over
            state-of-the-art LSTM models on standard benchmarks.
        </summary>
        <author>
            <name>Xiaodan Liang</name>
        </author>
        <author>
            <name>Liang Lin</name>
        </author>
        <author>
            <name>Xiaohui Shen</name>
        </author>
        <author>
            <name>Jiashi Feng</name>
        </author>
        <author>
            <name>Shuicheng Yan</name>
        </author>
        <author>
            <name>Eric P. Xing</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in CVPR 2017 as a spotlight paper</arxiv:comment>
        <link href="http://arxiv.org/abs/1703.03055v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1703.03055v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1611.05104v2</id>
        <updated>2016-12-17T06:47:05Z</updated>
        <published>2016-11-16T00:53:01Z</published>
        <title>A Way out of the Odyssey: Analyzing and Combining Recent Insights for
            LSTMs</title>
        <summary>  LSTMs have become a basic building block for many deep NLP models. In recent
            years, many improvements and variations have been proposed for deep sequence
            models in general, and LSTMs in particular. We propose and analyze a series of
            augmentations and modifications to LSTM networks resulting in improved
            performance for text classification datasets. We observe compounding
            improvements on traditional LSTMs using Monte Carlo test-time model averaging,
            average pooling, and residual connections, along with four other suggested
            modifications. Our analysis provides a simple, reliable, and high quality
            baseline model.
        </summary>
        <author>
            <name>Shayne Longpre</name>
        </author>
        <author>
            <name>Sabeek Pradhan</name>
        </author>
        <author>
            <name>Caiming Xiong</name>
        </author>
        <author>
            <name>Richard Socher</name>
        </author>
        <link href="http://arxiv.org/abs/1611.05104v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1611.05104v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1703.10722v2</id>
        <updated>2017-05-04T17:17:55Z</updated>
        <published>2017-03-31T00:50:37Z</published>
        <title>Factorization tricks for LSTM networks</title>
        <summary>  We present two simple ways of reducing the number of parameters and
            accelerating the training of large Long Short-Term Memory (LSTM) networks: the
            first one is "matrix factorization by design" of LSTM matrix into the product
            of two smaller matrices, and the second one is partitioning of LSTM matrix, its
            inputs and states into the independent groups. Both approaches allow us to
            train large LSTM networks significantly faster to the state-of the art
            perplexity. On the One Billion Word Benchmark we improve single model
            perplexity down to 23.36.
        </summary>
        <author>
            <name>Oleksii Kuchaiev</name>
        </author>
        <author>
            <name>Boris Ginsburg</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to ICLR 2017 Workshop</arxiv:comment>
        <link href="http://arxiv.org/abs/1703.10722v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1703.10722v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1704.00405v2</id>
        <updated>2017-04-20T01:55:26Z</updated>
        <published>2017-04-03T02:10:19Z</published>
        <title>Syntax Aware LSTM Model for Chinese Semantic Role Labeling</title>
        <summary>  As for semantic role labeling (SRL) task, when it comes to utilizing parsing
            information, both traditional methods and recent recurrent neural network (RNN)
            based methods use the feature engineering way. In this paper, we propose Syntax
            Aware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies
            according to dependency parsing information in order to model parsing
            information directly in an architecture engineering way instead of feature
            engineering way. We experimentally demonstrate that SA-LSTM gains more
            improvement from the model architecture. Furthermore, SA-LSTM outperforms the
            state-of-the-art on CPB 1.0 significantly according to Student t-test
            ($p&lt;0.05$).
        </summary>
        <author>
            <name>Feng Qian</name>
        </author>
        <author>
            <name>Lei Sha</name>
        </author>
        <author>
            <name>Baobao Chang</name>
        </author>
        <author>
            <name>Lu-chen Liu</name>
        </author>
        <author>
            <name>Ming Zhang</name>
        </author>
        <link href="http://arxiv.org/abs/1704.00405v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1704.00405v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1709.08432v1</id>
        <updated>2017-09-25T11:31:50Z</updated>
        <published>2017-09-25T11:31:50Z</published>
        <title>House Price Prediction Using LSTM</title>
        <summary>  In this paper, we use the house price data ranging from January 2004 to
            October 2016 to predict the average house price of November and December in
            2016 for each district in Beijing, Shanghai, Guangzhou and Shenzhen. We apply
            Autoregressive Integrated Moving Average model to generate the baseline while
            LSTM networks to build prediction model. These algorithms are compared in terms
            of Mean Squared Error. The result shows that the LSTM model has excellent
            properties with respect to predict time series. Also, stateful LSTM networks
            and stack LSTM networks are employed to further study the improvement of
            accuracy of the house prediction model.
        </summary>
        <author>
            <name>Xiaochen Chen</name>
        </author>
        <author>
            <name>Lai Wei</name>
        </author>
        <author>
            <name>Jiaxin Xu</name>
        </author>
        <link href="http://arxiv.org/abs/1709.08432v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1709.08432v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1604.06635v1</id>
        <updated>2016-04-22T12:51:11Z</updated>
        <published>2016-04-22T12:51:11Z</published>
        <title>Bridging LSTM Architecture and the Neural Dynamics during Reading</title>
        <summary>  Recently, the long short-term memory neural network (LSTM) has attracted wide
            interest due to its success in many tasks. LSTM architecture consists of a
            memory cell and three gates, which looks similar to the neuronal networks in
            the brain. However, there still lacks the evidence of the cognitive
            plausibility of LSTM architecture as well as its working mechanism. In this
            paper, we study the cognitive plausibility of LSTM by aligning its internal
            architecture with the brain activity observed via fMRI when the subjects read a
            story. Experiment results show that the artificial memory vector in LSTM can
            accurately predict the observed sequential brain activities, indicating the
            correlation between LSTM architecture and the cognitive process of story
            reading.
        </summary>
        <author>
            <name>Peng Qian</name>
        </author>
        <author>
            <name>Xipeng Qiu</name>
        </author>
        <author>
            <name>Xuanjing Huang</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25th International Joint Conference on Artificial Intelligence
            IJCAI-16</arxiv:comment>
        <link href="http://arxiv.org/abs/1604.06635v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1604.06635v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1709.04057v1</id>
        <updated>2017-09-12T20:52:22Z</updated>
        <published>2017-09-12T20:52:22Z</published>
        <title>Parallelizing Linear Recurrent Neural Nets Over Sequence Length</title>
        <summary>  Recurrent neural networks (RNNs) are widely used to model sequential data but
            their non-linear dependencies between sequence elements prevent parallelizing
            training over sequence length. We show the training of RNNs with only linear
            sequential dependencies can be parallelized over the sequence length using the
            parallel scan algorithm, leading to rapid training on long sequences with small
            minibatch size. We abstract prior linear sequence models into a new framework
            of linear surrogate RNNs and develop a linear surrogate long short-term memory
            (LS-LSTM) powered by a parallel linear recurrence CUDA kernel we implemented.
            We evaluate the LS-LSTM on a long sequence noisy autoregressive task and find
            the LS-LSTM achieves slightly superior train and test performance to a similar
            sized LSTM in 4x less training time. We analyze latency and throughput of the
            LS-LSTM and find the LS-LSTM reaches up to 175x the throughput of the LSTM in
            the small minibatch long sequence regime.
        </summary>
        <author>
            <name>Eric Martin</name>
        </author>
        <author>
            <name>Chris Cundy</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 2 Figures, 1 Table</arxiv:comment>
        <link href="http://arxiv.org/abs/1709.04057v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1709.04057v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1710.08744v1</id>
        <updated>2017-10-24T12:52:35Z</updated>
        <published>2017-10-24T12:52:35Z</published>
        <title>Online Training of LSTM Networks in Distributed Systems for Variable
            Length Data Sequences</title>
        <summary>  In this brief paper, we investigate online training of Long Short Term Memory
            (LSTM) architectures in a distributed network of nodes, where each node employs
            an LSTM based structure for online regression. In particular, each node
            sequentially receives a variable length data sequence with its label and can
            only exchange information with its neighbors to train the LSTM architecture. We
            first provide a generic LSTM based regression structure for each node. In order
            to train this structure, we put the LSTM equations in a nonlinear state space
            form for each node and then introduce a highly effective and efficient
            Distributed Particle Filtering (DPF) based training algorithm. We also
            introduce a Distributed Extended Kalman Filtering (DEKF) based training
            algorithm for comparison. Here, our DPF based training algorithm guarantees
            convergence to the performance of the optimal LSTM coefficients in the mean
            square error (MSE) sense under certain conditions. We achieve this performance
            with communication and computational complexity in the order of the first order
            gradient based methods. Through both simulated and real life examples, we
            illustrate significant performance improvements with respect to the state of
            the art methods.
        </summary>
        <author>
            <name>Tolga Ergen</name>
        </author>
        <author>
            <name>Suleyman Serdar Kozat</name>
        </author>
        <link href="http://arxiv.org/abs/1710.08744v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1710.08744v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
        <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1709.00489v1</id>
        <updated>2017-09-01T21:38:28Z</updated>
        <published>2017-09-01T21:38:28Z</published>
        <title>Arc-Standard Spinal Parsing with Stack-LSTMs</title>
        <summary>  We present a neural transition-based parser for spinal trees, a dependency
            representation of constituent trees. The parser uses Stack-LSTMs that compose
            constituent nodes with dependency-based derivations. In experiments, we show
            that this model adapts to different styles of dependency relations, but this
            choice has little effect for predicting constituent structure, suggesting that
            LSTMs induce useful states by themselves.
        </summary>
        <author>
            <name>Miguel Ballesteros</name>
        </author>
        <author>
            <name>Xavier Carreras</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IWPT 2017</arxiv:comment>
        <link href="http://arxiv.org/abs/1709.00489v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1709.00489v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1508.02774v1</id>
        <updated>2015-08-11T23:31:49Z</updated>
        <published>2015-08-11T23:31:49Z</published>
        <title>Benchmarking of LSTM Networks</title>
        <summary>  LSTM (Long Short-Term Memory) recurrent neural networks have been highly
            successful in a number of application areas. This technical report describes
            the use of the MNIST and UW3 databases for benchmarking LSTM networks and
            explores the effect of different architectural and hyperparameter choices on
            performance. Significant findings include: (1) LSTM performance depends
            smoothly on learning rates, (2) batching and momentum has no significant effect
            on performance, (3) softmax training outperforms least square training, (4)
            peephole units are not useful, (5) the standard non-linearities (tanh and
            sigmoid) perform best, (6) bidirectional training combined with CTC performs
            better than other methods.
        </summary>
        <author>
            <name>Thomas M. Breuel</name>
        </author>
        <link href="http://arxiv.org/abs/1508.02774v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1508.02774v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1605.01988v3</id>
        <updated>2017-03-30T18:24:55Z</updated>
        <published>2016-05-06T16:11:45Z</published>
        <title>LSTM with Working Memory</title>
        <summary>  Previous RNN architectures have largely been superseded by LSTM, or "Long
            Short-Term Memory". Since its introduction, there have been many variations on
            this simple design. However, it is still widely used and we are not aware of a
            gated-RNN architecture that outperforms LSTM in a broad sense while still being
            as simple and efficient. In this paper we propose a modified LSTM-like
            architecture. Our architecture is still simple and achieves better performance
            on the tasks that we tested on. We also introduce a new RNN performance
            benchmark that uses the handwritten digits and stresses several important
            network capabilities.
        </summary>
        <author>
            <name>Andrew Pulver</name>
        </author>
        <author>
            <name>Siwei Lyu</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IJCNN 2017</arxiv:comment>
        <link href="http://arxiv.org/abs/1605.01988v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1605.01988v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1611.06530v1</id>
        <updated>2016-11-20T15:39:43Z</updated>
        <published>2016-11-20T15:39:43Z</published>
        <title>Prototypical Recurrent Unit</title>
        <summary>  The difficulty in analyzing LSTM-like recurrent neural networks lies in the
            complex structure of the recurrent unit, which induces highly complex nonlinear
            dynamics. In this paper, we design a new simple recurrent unit, which we call
            Prototypical Recurrent Unit (PRU). We verify experimentally that PRU performs
            comparably to LSTM and GRU. This potentially enables PRU to be a prototypical
            example for analytic study of LSTM-like recurrent networks. Along these
            experiments, the memorization capability of LSTM-like networks is also studied
            and some insights are obtained.
        </summary>
        <author>
            <name>Dingkun Long</name>
        </author>
        <author>
            <name>Richong Zhang</name>
        </author>
        <author>
            <name>Yongyi Mao</name>
        </author>
        <link href="http://arxiv.org/abs/1611.06530v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1611.06530v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1612.03707v1</id>
        <updated>2016-12-12T14:36:22Z</updated>
        <published>2016-12-12T14:36:22Z</published>
        <title>Empirical Evaluation of A New Approach to Simplifying Long Short-term
            Memory (LSTM)</title>
        <summary>  The standard LSTM, although it succeeds in the modeling long-range
            dependences, suffers from a highly complex structure that can be simplified
            through modifications to its gate units. This paper was to perform an empirical
            comparison between the standard LSTM and three new simplified variants that
            were obtained by eliminating input signal, bias and hidden unit signal from
            individual gates, on the tasks of modeling two sequence datasets. The
            experiments show that the three variants, with reduced parameters, can achieve
            comparable performance with the standard LSTM. Due attention should be paid to
            turning the learning rate to achieve high accuracies
        </summary>
        <author>
            <name>Yuzhen Lu</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1612.03707v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1612.03707v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1701.03441v1</id>
        <updated>2017-01-12T18:12:05Z</updated>
        <published>2017-01-12T18:12:05Z</published>
        <title>Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural
            Networks</title>
        <summary>  The standard LSTM recurrent neural networks while very powerful in long-range
            dependency sequence applications have highly complex structure and relatively
            large (adaptive) parameters. In this work, we present empirical comparison
            between the standard LSTM recurrent neural network architecture and three new
            parameter-reduced variants obtained by eliminating combinations of the input
            signal, bias, and hidden unit signals from individual gating signals. The
            experiments on two sequence datasets show that the three new variants, called
            simply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the
            standard LSTM model with less (adaptive) parameters.
        </summary>
        <author>
            <name>Yuzhen Lu</name>
        </author>
        <author>
            <name>Fathi M. Salem</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 Figures, 3 Tables. arXiv admin note: substantial text
            overlap with arXiv:1612.03707</arxiv:comment>
        <link href="http://arxiv.org/abs/1701.03441v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1701.03441v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1703.00381v1</id>
        <updated>2017-03-01T16:50:54Z</updated>
        <published>2017-03-01T16:50:54Z</published>
        <title>The Statistical Recurrent Unit</title>
        <summary>  Sophisticated gated recurrent neural network architectures like LSTMs and
            GRUs have been shown to be highly effective in a myriad of applications. We
            develop an un-gated unit, the statistical recurrent unit (SRU), that is able to
            learn long term dependencies in data by only keeping moving averages of
            statistics. The SRU's architecture is simple, un-gated, and contains a
            comparable number of parameters to LSTMs; yet, SRUs perform favorably to more
            sophisticated LSTM and GRU alternatives, often outperforming one or both in
            various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an
            unbiased manner by optimizing respective architectures' hyperparameters in a
            Bayesian optimization scheme for both synthetic and real-world tasks.
        </summary>
        <author>
            <name>Junier B. Oliva</name>
        </author>
        <author>
            <name>Barnabas Poczos</name>
        </author>
        <author>
            <name>Jeff Schneider</name>
        </author>
        <link href="http://arxiv.org/abs/1703.00381v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1703.00381v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1704.06125v1</id>
        <updated>2017-04-20T13:10:25Z</updated>
        <published>2017-04-20T13:10:25Z</published>
        <title>BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and
            LSTMs</title>
        <summary>  In this paper we describe our attempt at producing a state-of-the-art Twitter
            sentiment classifier using Convolutional Neural Networks (CNNs) and Long Short
            Term Memory (LSTMs) networks. Our system leverages a large amount of unlabeled
            data to pre-train word embeddings. We then use a subset of the unlabeled data
            to fine tune the embeddings using distant supervision. The final CNNs and LSTMs
            are trained on the SemEval-2017 Twitter dataset where the embeddings are fined
            tuned again. To boost performances we ensemble several CNNs and LSTMs together.
            Our approach achieved first rank on all of the five English subtasks amongst 40
            teams.
        </summary>
        <author>
            <name>Mathieu Cliche</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of SemEval-2017, 8 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1704.06125v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1704.06125v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1706.01242v1</id>
        <updated>2017-06-05T09:04:07Z</updated>
        <published>2017-06-05T09:04:07Z</published>
        <title>Bayesian LSTMs in medicine</title>
        <summary>  The medical field stands to see significant benefits from the recent advances
            in deep learning. Knowing the uncertainty in the decision made by any machine
            learning algorithm is of utmost importance for medical practitioners. This
            study demonstrates the utility of using Bayesian LSTMs for classification of
            medical time series. Four medical time series datasets are used to show the
            accuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, we
            show cherry-picked examples of confident and uncertain classifications of the
            medical time series. With simple modifications of the common practice for deep
            learning, significant improvements can be made for the medical practitioner and
            patient.
        </summary>
        <author>
            <name>Jos van der Westhuizen</name>
        </author>
        <author>
            <name>Joan Lasenby</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1706.01242v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1706.01242v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1707.00722v1</id>
        <updated>2017-07-03T18:25:51Z</updated>
        <published>2017-07-03T18:25:51Z</published>
        <title>Improving LSTM-CTC based ASR performance in domains with limited
            training data</title>
        <summary>  This paper addresses the observed performance gap between automatic speech
            recognition (ASR) systems based on Long Short Term Memory (LSTM) neural
            networks trained with the connectionist temporal classification (CTC) loss
            function and systems based on hybrid Deep Neural Networks (DNNs) trained with
            the cross entropy (CE) loss function on domains with limited data. We step
            through a number of experiments that show incremental improvements on a
            baseline EESEN toolkit based LSTM-CTC ASR system trained on the Librispeech
            100hr (train-clean-100) corpus. Our results show that with effective
            combination of data augmentation and regularization, a LSTM-CTC based system
            can exceed the performance of a strong Kaldi based baseline trained on the same
            data.
        </summary>
        <author>
            <name>Jayadev Billa</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1707.00722v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1707.00722v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1703.10724v2</id>
        <updated>2017-06-20T01:22:18Z</updated>
        <published>2017-03-31T01:21:40Z</published>
        <title>N-gram Language Modeling using Recurrent Neural Network Estimation</title>
        <summary>  We investigate the effective memory depth of RNN models by using them for
            $n$-gram language model (LM) smoothing.
            Experiments on a small corpus (UPenn Treebank, one million words of training
            data and 10k vocabulary) have found the LSTM cell with dropout to be the best
            model for encoding the $n$-gram state when compared with feed-forward and
            vanilla RNN models. When preserving the sentence independence assumption the
            LSTM $n$-gram matches the LSTM LM performance for $n=9$ and slightly
            outperforms it for $n=13$. When allowing dependencies across sentence
            boundaries, the LSTM $13$-gram almost matches the perplexity of the unlimited
            history LSTM LM.
            LSTM $n$-gram smoothing also has the desirable property of improving with
            increasing $n$-gram order, unlike the Katz or Kneser-Ney back-off estimators.
            Using multinomial distributions as targets in training instead of the usual
            one-hot target is only slightly beneficial for low $n$-gram orders.
            Experiments on the One Billion Words benchmark show that the results hold at
            larger scale: while LSTM smoothing for short $n$-gram contexts does not provide
            significant advantages over classic N-gram models, it becomes effective with
            long contexts ($n &gt; 5$); depending on the task and amount of data it can match
            fully recurrent LSTM models at about $n=13$. This may have implications when
            modeling short-format text, e.g. voice search/query LMs.
            Building LSTM $n$-gram LMs may be appealing for some practical situations:
            the state in a $n$-gram LM can be succinctly represented with $(n-1)*4$ bytes
            storing the identity of the words in the context and batches of $n$-gram
            contexts can be processed in parallel. On the downside, the $n$-gram context
            encoding computed by the LSTM is discarded, making the model more expensive
            than a regular recurrent LSTM LM.
        </summary>
        <author>
            <name>Ciprian Chelba</name>
        </author>
        <author>
            <name>Mohammad Norouzi</name>
        </author>
        <author>
            <name>Samy Bengio</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, including references</arxiv:comment>
        <link href="http://arxiv.org/abs/1703.10724v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1703.10724v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1409.3215v3</id>
        <updated>2014-12-14T20:59:51Z</updated>
        <published>2014-09-10T19:55:35Z</published>
        <title>Sequence to Sequence Learning with Neural Networks</title>
        <summary>  Deep Neural Networks (DNNs) are powerful models that have achieved excellent
            performance on difficult learning tasks. Although DNNs work well whenever large
            labeled training sets are available, they cannot be used to map sequences to
            sequences. In this paper, we present a general end-to-end approach to sequence
            learning that makes minimal assumptions on the sequence structure. Our method
            uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to
            a vector of a fixed dimensionality, and then another deep LSTM to decode the
            target sequence from the vector. Our main result is that on an English to
            French translation task from the WMT'14 dataset, the translations produced by
            the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's
            BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did
            not have difficulty on long sentences. For comparison, a phrase-based SMT
            system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM
            to rerank the 1000 hypotheses produced by the aforementioned SMT system, its
            BLEU score increases to 36.5, which is close to the previous best result on
            this task. The LSTM also learned sensible phrase and sentence representations
            that are sensitive to word order and are relatively invariant to the active and
            the passive voice. Finally, we found that reversing the order of the words in
            all source sentences (but not target sentences) improved the LSTM's performance
            markedly, because doing so introduced many short term dependencies between the
            source and the target sentence which made the optimization problem easier.
        </summary>
        <author>
            <name>Ilya Sutskever</name>
        </author>
        <author>
            <name>Oriol Vinyals</name>
        </author>
        <author>
            <name>Quoc V. Le</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1409.3215v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1409.3215v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1603.07063v1</id>
        <updated>2016-03-23T03:31:02Z</updated>
        <published>2016-03-23T03:31:02Z</published>
        <title>Semantic Object Parsing with Graph LSTM</title>
        <summary>  By taking the semantic object parsing task as an exemplar application
            scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network,
            which is the generalization of LSTM from sequential data or multi-dimensional
            data to general graph-structured data. Particularly, instead of evenly and
            fixedly dividing an image to pixels or patches in existing multi-dimensional
            LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each
            arbitrary-shaped superpixel as a semantically consistent node, and adaptively
            construct an undirected graph for each image, where the spatial relations of
            the superpixels are naturally used as edges. Constructed on such an adaptive
            graph topology, the Graph LSTM is more naturally aligned with the visual
            patterns in the image (e.g., object boundaries or appearance similarities) and
            provides a more economical information propagation route. Furthermore, for each
            optimization step over Graph LSTM, we propose to use a confidence-driven scheme
            to update the hidden and memory states of nodes progressively till all nodes
            are updated. In addition, for each node, the forgets gates are adaptively
            learned to capture different degrees of semantic correlation with neighboring
            nodes. Comprehensive evaluations on four diverse semantic object parsing
            datasets well demonstrate the significant superiority of our Graph LSTM over
            other state-of-the-art solutions.
        </summary>
        <author>
            <name>Xiaodan Liang</name>
        </author>
        <author>
            <name>Xiaohui Shen</name>
        </author>
        <author>
            <name>Jiashi Feng</name>
        </author>
        <author>
            <name>Liang Lin</name>
        </author>
        <author>
            <name>Shuicheng Yan</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1603.07063v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1603.07063v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1710.03753v1</id>
        <updated>2017-10-10T14:09:22Z</updated>
        <published>2017-10-10T14:09:22Z</published>
        <title>Optimizing Long Short-Term Memory Recurrent Neural Networks Using Ant
            Colony Optimization to Predict Turbine Engine Vibration</title>
        <summary>  This article expands on research that has been done to develop a recurrent
            neural network (RNN) capable of predicting aircraft engine vibrations using
            long short-term memory (LSTM) neurons. LSTM RNNs can provide a more
            generalizable and robust method for prediction over analytical calculations of
            engine vibration, as analytical calculations must be solved iteratively based
            on specific empirical engine parameters, making this approach ungeneralizable
            across multiple engines. In initial work, multiple LSTM RNN architectures were
            proposed, evaluated and compared. This research improves the performance of the
            most effective LSTM network design proposed in the previous work by using a
            promising neuroevolution method based on ant colony optimization (ACO) to
            develop and enhance the LSTM cell structure of the network. A parallelized
            version of the ACO neuroevolution algorithm has been developed and the evolved
            LSTM RNNs were compared to the previously used fixed topology. The evolved
            networks were trained on a large database of flight data records obtained from
            an airline containing flights that suffered from excessive vibration. Results
            were obtained using MPI (Message Passing Interface) on a high performance
            computing (HPC) cluster, evolving 1000 different LSTM cell structures using 168
            cores over 4 days. The new evolved LSTM cells showed an improvement of 1.35%,
            reducing prediction error from 5.51% to 4.17% when predicting excessive engine
            vibrations 10 seconds in the future, while at the same time dramatically
            reducing the number of weights from 21,170 to 11,810.
        </summary>
        <author>
            <name>AbdElRahman ElSaid</name>
        </author>
        <author>
            <name>Travis Desell</name>
        </author>
        <author>
            <name>Fatima El Jamiy</name>
        </author>
        <author>
            <name>James Higgins</name>
        </author>
        <author>
            <name>Brandon Wild</name>
        </author>
        <link href="http://arxiv.org/abs/1710.03753v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1710.03753v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1503.00075v3</id>
        <updated>2015-05-30T06:51:20Z</updated>
        <published>2015-02-28T06:31:50Z</published>
        <title>Improved Semantic Representations From Tree-Structured Long Short-Term
            Memory Networks</title>
        <summary>  Because of their superior ability to preserve sequence information over time,
            Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with
            a more complex computational unit, have obtained strong results on a variety of
            sequence modeling tasks. The only underlying LSTM structure that has been
            explored so far is a linear chain. However, natural language exhibits syntactic
            properties that would naturally combine words to phrases. We introduce the
            Tree-LSTM, a generalization of LSTMs to tree-structured network topologies.
            Tree-LSTMs outperform all existing systems and strong LSTM baselines on two
            tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task
            1) and sentiment classification (Stanford Sentiment Treebank).
        </summary>
        <author>
            <name>Kai Sheng Tai</name>
        </author>
        <author>
            <name>Richard Socher</name>
        </author>
        <author>
            <name>Christopher D. Manning</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ACL 2015</arxiv:comment>
        <link href="http://arxiv.org/abs/1503.00075v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1503.00075v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1506.07452v1</id>
        <updated>2015-06-24T16:26:51Z</updated>
        <published>2015-06-24T16:26:51Z</published>
        <title>Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical
            Volumetric Image Segmentation</title>
        <summary>  Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D
            videos to segment them. They have a fixed input size and typically perceive
            only small local contexts of the pixels to be classified as foreground or
            background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive
            the entire spatio-temporal context of each pixel in a few sweeps through all
            pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite
            these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants
            were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid
            order of computations in MD-LSTM in pyramidal fashion. The resulting
            PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of
            brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image
            segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).
        </summary>
        <author>
            <name>Marijn F. Stollenga</name>
        </author>
        <author>
            <name>Wonmin Byeon</name>
        </author>
        <author>
            <name>Marcus Liwicki</name>
        </author>
        <author>
            <name>Juergen Schmidhuber</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Marijn F. Stollenga and Wonmin Byeon are the shared first authors,
            both authors contributed equally to this work</arxiv:comment>
        <link href="http://arxiv.org/abs/1506.07452v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1506.07452v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1507.01526v3</id>
        <updated>2016-01-07T18:39:48Z</updated>
        <published>2015-07-06T16:30:05Z</published>
        <title>Grid Long Short-Term Memory</title>
        <summary>  This paper introduces Grid Long Short-Term Memory, a network of LSTM cells
            arranged in a multidimensional grid that can be applied to vectors, sequences
            or higher dimensional data such as images. The network differs from existing
            deep LSTM architectures in that the cells are connected between network layers
            as well as along the spatiotemporal dimensions of the data. The network
            provides a unified way of using LSTM for both deep and sequential computation.
            We apply the model to algorithmic tasks such as 15-digit integer addition and
            sequence memorization, where it is able to significantly outperform the
            standard LSTM. We then give results for two empirical tasks. We find that 2D
            Grid LSTM achieves 1.47 bits per character on the Wikipedia character
            prediction benchmark, which is state-of-the-art among neural approaches. In
            addition, we use the Grid LSTM to define a novel two-dimensional translation
            model, the Reencoder, and show that it outperforms a phrase-based reference
            system on a Chinese-to-English translation task.
        </summary>
        <author>
            <name>Nal Kalchbrenner</name>
        </author>
        <author>
            <name>Ivo Danihelka</name>
        </author>
        <author>
            <name>Alex Graves</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1507.01526v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1507.01526v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1601.01530v4</id>
        <updated>2016-08-31T00:30:10Z</updated>
        <published>2016-01-07T13:32:31Z</published>
        <title>Leveraging Sentence-level Information with Encoder LSTM for Semantic
            Slot Filling</title>
        <summary>  Recurrent Neural Network (RNN) and one of its specific architectures, Long
            Short-Term Memory (LSTM), have been widely used for sequence labeling. In this
            paper, we first enhance LSTM-based sequence labeling to explicitly model label
            dependencies. Then we propose another enhancement to incorporate the global
            information spanning over the whole input sequence. The latter proposed method,
            encoder-labeler LSTM, first encodes the whole input sequence into a fixed
            length vector with the encoder LSTM, and then uses this encoded vector as the
            initial state of another LSTM for sequence labeling. Combining these methods,
            we can predict the label sequence with considering label dependencies and
            information of whole input sequence. In the experiments of a slot filling task,
            which is an essential component of natural language understanding, with using
            the standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.
        </summary>
        <author>
            <name>Gakuto Kurata</name>
        </author>
        <author>
            <name>Bing Xiang</name>
        </author>
        <author>
            <name>Bowen Zhou</name>
        </author>
        <author>
            <name>Mo Yu</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in EMNLP 2016</arxiv:comment>
        <link href="http://arxiv.org/abs/1601.01530v4" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1601.01530v4" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1606.01269v1</id>
        <updated>2016-06-03T20:32:52Z</updated>
        <published>2016-06-03T20:32:52Z</published>
        <title>End-to-end LSTM-based dialog control optimized with supervised and
            reinforcement learning</title>
        <summary>  This paper presents a model for end-to-end learning of task-oriented dialog
            systems. The main component of the model is a recurrent neural network (an
            LSTM), which maps from raw dialog history directly to a distribution over
            system actions. The LSTM automatically infers a representation of dialog
            history, which relieves the system developer of much of the manual feature
            engineering of dialog state. In addition, the developer can provide software
            that expresses business rules and provides access to programmatic APIs,
            enabling the LSTM to take actions in the real world on behalf of the user. The
            LSTM can be optimized using supervised learning (SL), where a domain expert
            provides example dialogs which the LSTM should imitate; or using reinforcement
            learning (RL), where the system improves by interacting directly with end
            users. Experiments show that SL and RL are complementary: SL alone can derive a
            reasonable initial policy from a small number of training dialogs; and starting
            RL optimization with a policy trained with SL substantially accelerates the
            learning rate of RL.
        </summary>
        <author>
            <name>Jason D. Williams</name>
        </author>
        <author>
            <name>Geoffrey Zweig</name>
        </author>
        <link href="http://arxiv.org/abs/1606.01269v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1606.01269v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1610.03165v1</id>
        <updated>2016-10-11T02:48:13Z</updated>
        <published>2016-10-11T02:48:13Z</published>
        <title>Long Short-Term Memory based Convolutional Recurrent Neural Networks for
            Large Vocabulary Speech Recognition</title>
        <summary>  Long short-term memory (LSTM) recurrent neural networks (RNNs) have been
            shown to give state-of-the-art performance on many speech recognition tasks, as
            they are able to provide the learned dynamically changing contextual window of
            all sequence history. On the other hand, the convolutional neural networks
            (CNNs) have brought significant improvements to deep feed-forward neural
            networks (FFNNs), as they are able to better reduce spectral variation in the
            input signal. In this paper, a network architecture called as convolutional
            recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN.
            In the proposed CRNNs, each speech frame, without adjacent context frames, is
            organized as a number of local feature patches along the frequency axis, and
            then a LSTM network is performed on each feature patch along the time axis. We
            train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various
            number of configurations. Experimental results show that the LSTM CRNNs can
            exceed state-of-the-art speech recognition performance.
        </summary>
        <author>
            <name>Xiangang Li</name>
        </author>
        <author>
            <name>Xihong Wu</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in INTERSPEECH 2015, September 6-10, 2015, Dresden, Germany</arxiv:comment>
        <link href="http://arxiv.org/abs/1610.03165v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1610.03165v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1610.09513v1</id>
        <updated>2016-10-29T14:05:10Z</updated>
        <published>2016-10-29T14:05:10Z</published>
        <title>Phased LSTM: Accelerating Recurrent Network Training for Long or
            Event-based Sequences</title>
        <summary>  Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for
            extracting patterns from temporal sequences. However, current RNN models are
            ill-suited to process irregularly sampled data triggered by events generated in
            continuous time by sensors or other neurons. Such data can occur, for example,
            when the input comes from novel event-driven artificial sensors that generate
            sparse, asynchronous streams of events or from multiple conventional sensors
            with different update intervals. In this work, we introduce the Phased LSTM
            model, which extends the LSTM unit by adding a new time gate. This gate is
            controlled by a parametrized oscillation with a frequency range that produces
            updates of the memory cell only during a small percentage of the cycle. Even
            with the sparse updates imposed by the oscillation, the Phased LSTM network
            achieves faster convergence than regular LSTMs on tasks which require learning
            of long sequences. The model naturally integrates inputs from sensors of
            arbitrary sampling rates, thereby opening new areas of investigation for
            processing asynchronous sensory events that carry timing information. It also
            greatly improves the performance of LSTMs in standard RNN applications, and
            does so with an order-of-magnitude fewer computes at runtime.
        </summary>
        <author>
            <name>Daniel Neil</name>
        </author>
        <author>
            <name>Michael Pfeiffer</name>
        </author>
        <author>
            <name>Shih-Chii Liu</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Selected for an oral presentation at NIPS, 2016</arxiv:comment>
        <link href="http://arxiv.org/abs/1610.09513v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1610.09513v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1611.05934v1</id>
        <updated>2016-11-18T00:13:32Z</updated>
        <published>2016-11-18T00:13:32Z</published>
        <title>Increasing the Interpretability of Recurrent Neural Networks Using
            Hidden Markov Models</title>
        <summary>  As deep neural networks continue to revolutionize various application
            domains, there is increasing interest in making these powerful models more
            understandable and interpretable, and narrowing down the causes of good and bad
            predictions. We focus on recurrent neural networks, state of the art models in
            speech recognition and translation. Our approach to increasing interpretability
            is by combining a long short-term memory (LSTM) model with a hidden Markov
            model (HMM), a simpler and more transparent model. We add the HMM state
            probabilities to the output layer of the LSTM, and then train the HMM and LSTM
            either sequentially or jointly. The LSTM can make use of the information from
            the HMM, and fill in the gaps when the HMM is not performing well. A small
            hybrid model usually performs better than a standalone LSTM of the same size,
            especially on smaller data sets. We test the algorithms on text data and
            medical time series data, and find that the LSTM and HMM learn complementary
            information about the features in the text.
        </summary>
        <author>
            <name>Viktoriya Krakovna</name>
        </author>
        <author>
            <name>Finale Doshi-Velez</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at NIPS 2016 Workshop on Interpretable Machine Learning in
            Complex Systems. arXiv admin note: substantial text overlap with
            arXiv:1606.05320</arxiv:comment>
        <link href="http://arxiv.org/abs/1611.05934v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1611.05934v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1611.06788v1</id>
        <updated>2016-11-21T14:01:53Z</updated>
        <published>2016-11-21T14:01:53Z</published>
        <title>Bidirectional Tree-Structured LSTM with Head Lexicalization</title>
        <summary>  Sequential LSTM has been extended to model tree structures, giving
            competitive results for a number of tasks. Existing methods model constituent
            trees by bottom-up combinations of constituent nodes, making direct use of
            input word information only for leaf nodes. This is different from sequential
            LSTMs, which contain reference to input words for each node. In this paper, we
            propose a method for automatic head-lexicalization for tree-structure LSTMs,
            propagating head words from leaf nodes to every constituent node. In addition,
            enabled by head lexicalization, we build a tree LSTM in the top-down direction,
            which corresponds to bidirectional sequential LSTM structurally. Experiments
            show that both extensions give better representations of tree structures. Our
            final model gives the best results on the Standford Sentiment Treebank and
            highly competitive results on the TREC question type classification task.
        </summary>
        <author>
            <name>Zhiyang Teng</name>
        </author>
        <author>
            <name>Yue Zhang</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1611.06788v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1611.06788v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1702.03402v1</id>
        <updated>2017-02-11T09:50:40Z</updated>
        <published>2017-02-11T09:50:40Z</published>
        <title>Parallel Long Short-Term Memory for Multi-stream Classification</title>
        <summary>  Recently, machine learning methods have provided a broad spectrum of original
            and efficient algorithms based on Deep Neural Networks (DNN) to automatically
            predict an outcome with respect to a sequence of inputs. Recurrent hidden cells
            allow these DNN-based models to manage long-term dependencies such as Recurrent
            Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these
            RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM)
            directions. But most of the information available nowadays is from multistreams
            or multimedia documents, and require RNNs to process these information
            synchronously during the training. This paper presents an original LSTM-based
            architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel
            synchronized input sequences in order to predict a common output. The proposed
            PLSTM method could be used for parallel sequence classification purposes. The
            PLSTM approach is evaluated on an automatic telecast genre sequences
            classification task and compared with different state-of-the-art architectures.
            Results show that the proposed PLSTM method outperforms the baseline n-gram
            models as well as the state-of-the-art LSTM approach.
        </summary>
        <author>
            <name>Mohamed Bouaziz</name>
        </author>
        <author>
            <name>Mohamed Morchid</name>
        </author>
        <author>
            <name>Richard Dufour</name>
        </author>
        <author>
            <name>Georges Linars</name>
        </author>
        <author>
            <name>Renato De Mori</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2016 IEEE Workshop on Spoken Language Technology</arxiv:comment>
        <link href="http://arxiv.org/abs/1702.03402v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1702.03402v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1703.09370v1</id>
        <updated>2017-03-28T02:00:47Z</updated>
        <published>2017-03-28T02:00:47Z</published>
        <title>Ensembles of Deep LSTM Learners for Activity Recognition using Wearables</title>
        <summary>  Recently, deep learning (DL) methods have been introduced very successfully
            into human activity recognition (HAR) scenarios in ubiquitous and wearable
            computing. Especially the prospect of overcoming the need for manual feature
            design combined with superior classification capabilities render deep neural
            networks very attractive for real-life HAR application. Even though DL-based
            approaches now outperform the state-of-the-art in a number of recognitions
            tasks of the field, yet substantial challenges remain. Most prominently, issues
            with real-life datasets, typically including imbalanced datasets and
            problematic data quality, still limit the effectiveness of activity recognition
            using wearables. In this paper we tackle such challenges through Ensembles of
            deep Long Short Term Memory (LSTM) networks. We have developed modified
            training procedures for LSTM networks and combine sets of diverse LSTM learners
            into classifier collectives. We demonstrate, both formally and empirically,
            that Ensembles of deep LSTM learners outperform the individual LSTM networks.
            Through an extensive experimental evaluation on three standard benchmarks
            (Opportunity, PAMAP2, Skoda) we demonstrate the excellent recognition
            capabilities of our approach and its potential for real-life applications of
            human activity recognition.
        </summary>
        <author>
            <name>Yu Guan</name>
        </author>
        <author>
            <name>Thomas Ploetz</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication in ACM IMWUT (Ubicomp) 2017</arxiv:comment>
        <link href="http://arxiv.org/abs/1703.09370v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1703.09370v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1705.08153v1</id>
        <updated>2017-05-23T09:35:39Z</updated>
        <published>2017-05-23T09:35:39Z</published>
        <title>Visualizing LSTM decisions</title>
        <summary>  Long Short-Term Memory (LSTM) recurrent neural networks are renowned for
            being uninterpretable "black boxes". In the medical domain where LSTMs have
            shown promise, this is specifically concerning because it is imperative to
            understand the decisions made by machine learning models in such acute
            situations. This study employs techniques used in the Convolutional Neural
            Network domain to elucidate the operations that LSTMs perform on time series.
            The visualization techniques include input saliency by means of occlusion and
            derivatives, class mode visualization, and temporal outputs. Moreover, we
            demonstrate that LSTMs appear to extract features similar to those extracted by
            wavelets. It was found that deriving the inputs for saliency is a poor
            approximation and occlusion is a better approach. Moreover, analyzing LSTMs on
            different sets of data provide novel interpretations.
        </summary>
        <author>
            <name>Jos van der Westhuizen</name>
        </author>
        <author>
            <name>Joan Lasenby</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1705.08153v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1705.08153v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1707.02356v1</id>
        <updated>2017-07-06T11:06:26Z</updated>
        <published>2017-07-06T11:06:26Z</published>
        <title>Skeleton-based Action Recognition Using LSTM and CNN</title>
        <summary>  Recent methods based on 3D skeleton data have achieved outstanding
            performance due to its conciseness, robustness, and view-independent
            representation. With the development of deep learning, Convolutional Neural
            Networks (CNN) and Long Short Term Memory (LSTM)-based learning methods have
            achieved promising performance for action recognition. However, for CNN-based
            methods, it is inevitable to loss temporal information when a sequence is
            encoded into images. In order to capture as much spatial-temporal information
            as possible, LSTM and CNN are adopted to conduct effective recognition with
            later score fusion. In addition, experimental results show that the score
            fusion between CNN and LSTM performs better than that between LSTM and LSTM for
            the same feature. Our method achieved state-of-the-art results on NTU RGB+D
            datasets for 3D human action analysis. The proposed method achieved 87.40% in
            terms of accuracy and ranked $1^{st}$ place in Large Scale 3D Human Activity
            Analysis Challenge in Depth Videos.
        </summary>
        <author>
            <name>Chuankun Li</name>
        </author>
        <author>
            <name>Pichao Wang</name>
        </author>
        <author>
            <name>Shuang Wang</name>
        </author>
        <author>
            <name>Yonghong Hou</name>
        </author>
        <author>
            <name>Wanqing Li</name>
        </author>
        <link href="http://arxiv.org/abs/1707.02356v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1707.02356v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1707.06372v1</id>
        <updated>2017-07-20T04:47:00Z</updated>
        <published>2017-07-20T04:47:00Z</published>
        <title>Learning to Rank Question Answer Pairs with Holographic Dual LSTM
            Architecture</title>
        <summary>  We describe a new deep learning architecture for learning to rank question
            answer pairs. Our approach extends the long short-term memory (LSTM) network
            with holographic composition to model the relationship between question and
            answer representations. As opposed to the neural tensor layer that has been
            adopted recently, the holographic composition provides the benefits of scalable
            and rich representational learning approach without incurring huge parameter
            costs. Overall, we present Holographic Dual LSTM (HD-LSTM), a unified
            architecture for both deep sentence modeling and semantic matching.
            Essentially, our model is trained end-to-end whereby the parameters of the LSTM
            are optimized in a way that best explains the correlation between question and
            answer representations. In addition, our proposed deep learning architecture
            requires no extensive feature engineering. Via extensive experiments, we show
            that HD-LSTM outperforms many other neural architectures on two popular
            benchmark QA datasets. Empirical studies confirm the effectiveness of
            holographic composition over the neural tensor layer.
        </summary>
        <author>
            <name>Yi Tay</name>
        </author>
        <author>
            <name>Minh C. Phan</name>
        </author>
        <author>
            <name>Luu Anh Tuan</name>
        </author>
        <author>
            <name>Siu Cheung Hui</name>
        </author>
        <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3077136.3080790</arxiv:doi>
        <link title="doi" href="http://dx.doi.org/10.1145/3077136.3080790" rel="related"/>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2017 Full Paper</arxiv:comment>
        <link href="http://arxiv.org/abs/1707.06372v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1707.06372v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1708.09011v1</id>
        <updated>2017-08-22T18:59:22Z</updated>
        <published>2017-08-22T18:59:22Z</published>
        <title>Real-Time Pose Estimation for Event Cameras with Stacked Spatial LSTM
            Networks</title>
        <summary>  We present a new method to estimate the 6DOF pose of the event camera solely
            based on the event stream. Our method first creates the event image from a list
            of events that occurs in a very short time interval, then a Stacked Spatial
            LSTM Network (SP-LSTM) is used to learn and estimate the camera pose. Our
            SP-LSTM comprises a CNN to learn deep features from the event images and a
            stack of LSTM to learn spatial dependencies in the image features space. We
            show that the spatial dependency plays an important role in the pose estimation
            task and the SP-LSTM can effectively learn that information. The experimental
            results on the public dataset show that our approach outperforms recent methods
            by a substantial margin. Overall, our proposed method reduces about 6 times the
            position error and 3 times the orientation error over the state of the art. The
            source code and trained models will be released.
        </summary>
        <author>
            <name>Anh Nguyen</name>
        </author>
        <author>
            <name>Thanh-Toan Do</name>
        </author>
        <author>
            <name>Darwin G. Caldwell</name>
        </author>
        <author>
            <name>Nikos G. Tsagarakis</name>
        </author>
        <link href="http://arxiv.org/abs/1708.09011v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1708.09011v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1503.04069v2</id>
        <updated>2017-10-04T11:40:31Z</updated>
        <published>2015-03-13T14:01:38Z</published>
        <title>LSTM: A Search Space Odyssey</title>
        <summary>  Several variants of the Long Short-Term Memory (LSTM) architecture for
            recurrent neural networks have been proposed since its inception in 1995. In
            recent years, these networks have become the state-of-the-art models for a
            variety of machine learning problems. This has led to a renewed interest in
            understanding the role and utility of various computational components of
            typical LSTM variants. In this paper, we present the first large-scale analysis
            of eight LSTM variants on three representative tasks: speech recognition,
            handwriting recognition, and polyphonic music modeling. The hyperparameters of
            all LSTM variants for each task were optimized separately using random search,
            and their importance was assessed using the powerful fANOVA framework. In
            total, we summarize the results of 5400 experimental runs ($\approx 15$ years
            of CPU time), which makes our study the largest of its kind on LSTM networks.
            Our results show that none of the variants can improve upon the standard LSTM
            architecture significantly, and demonstrate the forget gate and the output
            activation function to be its most critical components. We further observe that
            the studied hyperparameters are virtually independent and derive guidelines for
            their efficient adjustment.
        </summary>
        <author>
            <name>Klaus Greff</name>
        </author>
        <author>
            <name>Rupesh Kumar Srivastava</name>
        </author>
        <author>
            <name>Jan Koutnk</name>
        </author>
        <author>
            <name>Bas R. Steunebrink</name>
        </author>
        <author>
            <name>Jrgen Schmidhuber</name>
        </author>
        <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2016.2582924</arxiv:doi>
        <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2016.2582924" rel="related"/>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
        <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks and Learning Systems (
            Volume: 28, Issue: 10, Oct. 2017 ) Pages: 2222 - 2232</arxiv:journal_ref>
        <link href="http://arxiv.org/abs/1503.04069v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1503.04069v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
        <category term="I.2.6; I.2.7; I.5.1; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1709.05027v3</id>
        <updated>2017-10-28T15:49:22Z</updated>
        <published>2017-09-15T01:10:23Z</published>
        <title>Learning Intrinsic Sparse Structures within Long Short-Term Memory</title>
        <summary>  Model compression is significant for the wide adoption of Recurrent Neural
            Networks (RNNs) in both user devices possessing limited resources and business
            clusters requiring quick responses to large-scale service requests. This work
            aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the
            sizes of basic structures within LSTM units, including input updates, gates,
            hidden states, cell states and outputs. Independently reducing the sizes of
            basic structures can result in inconsistent dimensions among them, and
            consequently, end up with invalid LSTM units. To overcome the problem, we
            propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS
            will decrease the sizes of all basic structures by one simultaneously and
            thereby always maintain the dimension consistency. By learning ISS within LSTM
            units, the obtained LSTMs remain regular while having much smaller basic
            structures. Our method achieves 10.59x speedup in state-of-the-art LSTMs,
            without losing any perplexity of language modeling of Penn TreeBank dataset. It
            is also successfully evaluated through a compact model with only 2.69M weights
            for machine Question Answering of SQuAD dataset. Our source code is public
            available at https://github.com/wenwei202/iss-rnns
        </summary>
        <author>
            <name>Wei Wen</name>
        </author>
        <author>
            <name>Yuxiong He</name>
        </author>
        <author>
            <name>Samyam Rajbhandari</name>
        </author>
        <author>
            <name>Wenhan Wang</name>
        </author>
        <author>
            <name>Fang Liu</name>
        </author>
        <author>
            <name>Bin Hu</name>
        </author>
        <author>
            <name>Yiran Chen</name>
        </author>
        <author>
            <name>Hai Li</name>
        </author>
        <link href="http://arxiv.org/abs/1709.05027v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1709.05027v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1410.4281v2</id>
        <updated>2015-05-11T02:23:06Z</updated>
        <published>2014-10-16T02:44:41Z</published>
        <title>Constructing Long Short-Term Memory based Deep Recurrent Neural Networks
            for Large Vocabulary Speech Recognition</title>
        <summary>  Long short-term memory (LSTM) based acoustic modeling methods have recently
            been shown to give state-of-the-art performance on some speech recognition
            tasks. To achieve a further performance improvement, in this research, deep
            extensions on LSTM are investigated considering that deep hierarchical model
            has turned out to be more efficient than a shallow one. Motivated by previous
            research on constructing deep recurrent neural networks (RNNs), alternative
            deep LSTM architectures are proposed and empirically evaluated on a large
            vocabulary conversational telephone speech recognition task. Meanwhile,
            regarding to multi-GPU devices, the training process for LSTM networks is
            introduced and discussed. Experimental results demonstrate that the deep LSTM
            networks benefit from the depth and yield the state-of-the-art performance on
            this task.
        </summary>
        <author>
            <name>Xiangang Li</name>
        </author>
        <author>
            <name>Xihong Wu</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICASSP 2015 which does not perform blind reviews</arxiv:comment>
        <link href="http://arxiv.org/abs/1410.4281v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1410.4281v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1503.01919v1</id>
        <updated>2015-03-06T11:21:26Z</updated>
        <published>2015-03-06T11:21:26Z</published>
        <title>Convolutional LSTM Networks for Subcellular Localization of Proteins</title>
        <summary>  Machine learning is widely used to analyze biological sequence data.
            Non-sequential models such as SVMs or feed-forward neural networks are often
            used although they have no natural way of handling sequences of varying length.
            Recurrent neural networks such as the long short term memory (LSTM) model on
            the other hand are designed to handle sequences. In this study we demonstrate
            that LSTM networks predict the subcellular location of proteins given only the
            protein sequence with high accuracy (0.902) outperforming current state of the
            art algorithms. We further improve the performance by introducing convolutional
            filters and experiment with an attention mechanism which lets the LSTM focus on
            specific parts of the protein. Lastly we introduce new visualizations of both
            the convolutional filters and the attention mechanisms and show how they can be
            used to extract biological relevant knowledge from the LSTM networks.
        </summary>
        <author>
            <name>Sren Kaae Snderby</name>
        </author>
        <author>
            <name>Casper Kaae Snderby</name>
        </author>
        <author>
            <name>Henrik Nielsen</name>
        </author>
        <author>
            <name>Ole Winther</name>
        </author>
        <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-21233-3_6</arxiv:doi>
        <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-21233-3_6" rel="related"/>
        <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Algorithms for Computational Biology 9199 (2015) 68</arxiv:journal_ref>
        <link href="http://arxiv.org/abs/1503.01919v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1503.01919v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
        <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1412.2620v2</id>
        <updated>2016-02-16T12:26:37Z</updated>
        <published>2014-12-08T15:47:45Z</published>
        <title>Cells in Multidimensional Recurrent Neural Networks</title>
        <summary>  The transcription of handwritten text on images is one task in machine
            learning and one solution to solve it is using multi-dimensional recurrent
            neural networks (MDRNN) with connectionist temporal classification (CTC). The
            RNNs can contain special units, the long short-term memory (LSTM) cells. They
            are able to learn long term dependencies but they get unstable when the
            dimension is chosen greater than one. We defined some useful and necessary
            properties for the one-dimensional LSTM cell and extend them in the
            multi-dimensional case. Thereby we introduce several new cells with better
            stability. We present a method to design cells using the theory of linear shift
            invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT
            and Rimes database, where we can improve the recognition rate compared to the
            LSTM cell. So each application where the LSTM cells in MDRNNs are used could be
            improved by substituting them by the new developed cells.
        </summary>
        <author>
            <name>G. Leifert</name>
            <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock</arxiv:affiliation>
        </author>
        <author>
            <name>T. Strau</name>
            <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock</arxiv:affiliation>
        </author>
        <author>
            <name>T. Grning</name>
            <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock</arxiv:affiliation>
        </author>
        <author>
            <name>R. Labahn</name>
            <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock</arxiv:affiliation>
        </author>
        <link href="http://arxiv.org/abs/1412.2620v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1412.2620v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="68T10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1504.06678v1</id>
        <updated>2015-04-25T03:59:14Z</updated>
        <published>2015-04-25T03:59:14Z</published>
        <title>Differential Recurrent Neural Networks for Action Recognition</title>
        <summary>  The long short-term memory (LSTM) neural network is capable of processing
            complex sequential information since it utilizes special gating schemes for
            learning representations from long input sequences. It has the potential to
            model any sequential time-series data, where the current hidden state has to be
            considered in the context of the past hidden states. This property makes LSTM
            an ideal choice to learn the complex dynamics of various actions.
            Unfortunately, the conventional LSTMs do not consider the impact of
            spatio-temporal dynamics corresponding to the given salient motion patterns,
            when they gate the information that ought to be memorized through time. To
            address this problem, we propose a differential gating scheme for the LSTM
            neural network, which emphasizes on the change in information gain caused by
            the salient motions between the successive frames. This change in information
            gain is quantified by Derivative of States (DoS), and thus the proposed LSTM
            model is termed as differential Recurrent Neural Network (dRNN). We demonstrate
            the effectiveness of the proposed model by automatically recognizing actions
            from the real-world 2D and 3D human action datasets. Our study is one of the
            first works towards demonstrating the potential of learning complex time-series
            representations via high-order derivatives of states.
        </summary>
        <author>
            <name>Vivek Veeriah</name>
        </author>
        <author>
            <name>Naifan Zhuang</name>
        </author>
        <author>
            <name>Guo-Jun Qi</name>
        </author>
        <link href="http://arxiv.org/abs/1504.06678v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1504.06678v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1506.04214v2</id>
        <updated>2015-09-19T11:02:03Z</updated>
        <published>2015-06-13T03:19:24Z</published>
        <title>Convolutional LSTM Network: A Machine Learning Approach for
            Precipitation Nowcasting</title>
        <summary>  The goal of precipitation nowcasting is to predict the future rainfall
            intensity in a local region over a relatively short period of time. Very few
            previous studies have examined this crucial and challenging weather forecasting
            problem from the machine learning perspective. In this paper, we formulate
            precipitation nowcasting as a spatiotemporal sequence forecasting problem in
            which both the input and the prediction target are spatiotemporal sequences. By
            extending the fully connected LSTM (FC-LSTM) to have convolutional structures
            in both the input-to-state and state-to-state transitions, we propose the
            convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model
            for the precipitation nowcasting problem. Experiments show that our ConvLSTM
            network captures spatiotemporal correlations better and consistently
            outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for
            precipitation nowcasting.
        </summary>
        <author>
            <name>Xingjian Shi</name>
        </author>
        <author>
            <name>Zhourong Chen</name>
        </author>
        <author>
            <name>Hao Wang</name>
        </author>
        <author>
            <name>Dit-Yan Yeung</name>
        </author>
        <author>
            <name>Wai-kin Wong</name>
        </author>
        <author>
            <name>Wang-chun Woo</name>
        </author>
        <link href="http://arxiv.org/abs/1506.04214v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1506.04214v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1512.01100v2</id>
        <updated>2016-09-29T09:40:39Z</updated>
        <published>2015-12-03T14:54:39Z</published>
        <title>Effective LSTMs for Target-Dependent Sentiment Classification</title>
        <summary>  Target-dependent sentiment classification remains a challenge: modeling the
            semantic relatedness of a target with its context words in a sentence.
            Different context words have different influences on determining the sentiment
            polarity of a sentence towards the target. Therefore, it is desirable to
            integrate the connections between target word and context words when building a
            learning system. In this paper, we develop two target dependent long short-term
            memory (LSTM) models, where target information is automatically taken into
            account. We evaluate our methods on a benchmark dataset from Twitter. Empirical
            results show that modeling sentence representation with standard LSTM does not
            perform well. Incorporating target information into LSTM can significantly
            boost the classification accuracy. The target-dependent LSTM models achieve
            state-of-the-art performances without using syntactic parser or external
            sentiment lexicons.
        </summary>
        <author>
            <name>Duyu Tang</name>
        </author>
        <author>
            <name>Bing Qin</name>
        </author>
        <author>
            <name>Xiaocheng Feng</name>
        </author>
        <author>
            <name>Ting Liu</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures published in COLING 2016</arxiv:comment>
        <link href="http://arxiv.org/abs/1512.01100v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1512.01100v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1512.08849v2</id>
        <updated>2016-11-10T11:54:29Z</updated>
        <published>2015-12-30T05:02:53Z</published>
        <title>Learning Natural Language Inference with LSTM</title>
        <summary>  Natural language inference (NLI) is a fundamentally important task in natural
            language processing that has many applications. The recently released Stanford
            Natural Language Inference (SNLI) corpus has made it possible to develop and
            evaluate learning-centered methods such as deep neural networks for natural
            language inference (NLI). In this paper, we propose a special long short-term
            memory (LSTM) architecture for NLI. Our model builds on top of a recently
            proposed neural attention model for NLI but is based on a significantly
            different idea. Instead of deriving sentence embeddings for the premise and the
            hypothesis to be used for classification, our solution uses a match-LSTM to
            perform word-by-word matching of the hypothesis with the premise. This LSTM is
            able to place more emphasis on important word-level matching results. In
            particular, we observe that this LSTM remembers important mismatches that are
            critical for predicting the contradiction or the neutral relationship label. On
            the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the
            state of the art.
        </summary>
        <author>
            <name>Shuohang Wang</name>
        </author>
        <author>
            <name>Jing Jiang</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1512.08849v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1512.08849v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1602.02373v2</id>
        <updated>2016-05-26T15:26:34Z</updated>
        <published>2016-02-07T14:05:58Z</published>
        <title>Supervised and Semi-Supervised Text Categorization using LSTM for Region
            Embeddings</title>
        <summary>  One-hot CNN (convolutional neural network) has been shown to be effective for
            text categorization (Johnson &amp; Zhang, 2015). We view it as a special case of a
            general framework which jointly trains a linear model with a non-linear feature
            generator consisting of `text region embedding + pooling'. Under this
            framework, we explore a more sophisticated region embedding method using Long
            Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly
            large) sizes, whereas the region size needs to be fixed in a CNN. We seek
            effective and efficient use of LSTM for this purpose in the supervised and
            semi-supervised settings. The best results were obtained by combining region
            embeddings in the form of LSTM and convolution layers trained on unlabeled
            data. The results indicate that on this task, embeddings of text regions, which
            can convey complex concepts, are more useful than embeddings of single words in
            isolation. We report performances exceeding the previous best results on four
            benchmark datasets.
        </summary>
        <author>
            <name>Rie Johnson</name>
        </author>
        <author>
            <name>Tong Zhang</name>
        </author>
        <link href="http://arxiv.org/abs/1602.02373v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1602.02373v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1602.04364v1</id>
        <updated>2016-02-13T18:49:50Z</updated>
        <published>2016-02-13T18:49:50Z</published>
        <title>Look, Listen and Learn - A Multimodal LSTM for Speaker Identification</title>
        <summary>  Speaker identification refers to the task of localizing the face of a person
            who has the same identity as the ongoing voice in a video. This task not only
            requires collective perception over both visual and auditory signals, the
            robustness to handle severe quality degradations and unconstrained content
            variations are also indispensable. In this paper, we describe a novel
            multimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifies
            both visual and auditory modalities from the beginning of each sequence input.
            The key idea is to extend the conventional LSTM by not only sharing weights
            across time steps, but also sharing weights across modalities. We show that
            modeling the temporal dependency across face and voice can significantly
            improve the robustness to content quality degradations and variations. We also
            found that our multimodal LSTM is robustness to distractors, namely the
            non-speaking identities. We applied our multimodal LSTM to The Big Bang Theory
            dataset and showed that our system outperforms the state-of-the-art systems in
            speaker identification with lower false alarm rate and higher recognition
            accuracy.
        </summary>
        <author>
            <name>Jimmy Ren</name>
        </author>
        <author>
            <name>Yongtao Hu</name>
        </author>
        <author>
            <name>Yu-Wing Tai</name>
        </author>
        <author>
            <name>Chuan Wang</name>
        </author>
        <author>
            <name>Li Xu</name>
        </author>
        <author>
            <name>Wenxiu Sun</name>
        </author>
        <author>
            <name>Qiong Yan</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 30th AAAI Conference on Artificial Intelligence (AAAI-16)</arxiv:comment>
        <link href="http://arxiv.org/abs/1602.04364v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1602.04364v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1603.07772v1</id>
        <updated>2016-03-24T22:43:55Z</updated>
        <published>2016-03-24T22:43:55Z</published>
        <title>Co-occurrence Feature Learning for Skeleton based Action Recognition
            using Regularized Deep LSTM Networks</title>
        <summary>  Skeleton based action recognition distinguishes human actions using the
            trajectories of skeleton joints, which provide a very good representation for
            describing actions. Considering that recurrent neural networks (RNNs) with Long
            Short-Term Memory (LSTM) can learn feature representations and model long-term
            temporal dependencies automatically, we propose an end-to-end fully connected
            deep LSTM network for skeleton based action recognition. Inspired by the
            observation that the co-occurrences of the joints intrinsically characterize
            human actions, we take the skeleton as the input at each time slot and
            introduce a novel regularization scheme to learn the co-occurrence features of
            skeleton joints. To train the deep LSTM network effectively, we propose a new
            dropout algorithm which simultaneously operates on the gates, cells, and output
            responses of the LSTM neurons. Experimental results on three human action
            recognition datasets consistently demonstrate the effectiveness of the proposed
            model.
        </summary>
        <author>
            <name>Wentao Zhu</name>
        </author>
        <author>
            <name>Cuiling Lan</name>
        </author>
        <author>
            <name>Junliang Xing</name>
        </author>
        <author>
            <name>Wenjun Zeng</name>
        </author>
        <author>
            <name>Yanghao Li</name>
        </author>
        <author>
            <name>Li Shen</name>
        </author>
        <author>
            <name>Xiaohui Xie</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2016 conference</arxiv:comment>
        <link href="http://arxiv.org/abs/1603.07772v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1603.07772v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1604.05529v3</id>
        <updated>2016-07-21T08:17:43Z</updated>
        <published>2016-04-19T11:53:09Z</published>
        <title>Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term
            Memory Models and Auxiliary Loss</title>
        <summary>  Bidirectional long short-term memory (bi-LSTM) networks have recently proven
            successful for various NLP sequence modeling tasks, but little is known about
            their reliance to input representations, target languages, data set size, and
            label noise. We address these issues and evaluate bi-LSTMs with word,
            character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to
            traditional POS taggers across languages and data sizes. We also present a
            novel bi-LSTM model, which combines the POS tagging loss function with an
            auxiliary loss function that accounts for rare words. The model obtains
            state-of-the-art performance across 22 languages, and works especially well for
            morphologically complex languages. Our analysis suggests that bi-LSTMs are less
            sensitive to training data size and label corruptions (at small noise levels)
            than previously assumed.
        </summary>
        <author>
            <name>Barbara Plank</name>
        </author>
        <author>
            <name>Anders Sgaard</name>
        </author>
        <author>
            <name>Yoav Goldberg</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In ACL 2016 (short)</arxiv:comment>
        <link href="http://arxiv.org/abs/1604.05529v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1604.05529v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1606.06061v2</id>
        <updated>2016-06-22T15:11:30Z</updated>
        <published>2016-06-20T10:54:51Z</published>
        <title>Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric
            Speech Synthesizers for Mobile Devices</title>
        <summary>  Acoustic models based on long short-term memory recurrent neural networks
            (LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and
            showed significant improvements in naturalness and latency over those based on
            hidden Markov models (HMMs). This paper describes further optimizations of
            LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization,
            multi-frame inference, and robust inference using an {\epsilon}-contaminated
            Gaussian loss function. Experimental results in subjective listening tests show
            that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based
            SPSS in runtime speed while maintaining naturalness. Evaluations between
            LSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also
            presented.
        </summary>
        <author>
            <name>Heiga Zen</name>
        </author>
        <author>
            <name>Yannis Agiomyrgiannakis</name>
        </author>
        <author>
            <name>Niels Egberts</name>
        </author>
        <author>
            <name>Fergus Henderson</name>
        </author>
        <author>
            <name>Przemysaw Szczepaniak</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures, Interspeech 2016 (accepted)</arxiv:comment>
        <link href="http://arxiv.org/abs/1606.06061v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1606.06061v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1609.07959v3</id>
        <updated>2017-10-12T17:05:47Z</updated>
        <published>2016-09-26T13:12:51Z</published>
        <title>Multiplicative LSTM for sequence modelling</title>
        <summary>  We introduce multiplicative LSTM (mLSTM), a recurrent neural network
            architecture for sequence modelling that combines the long short-term memory
            (LSTM) and multiplicative recurrent neural network architectures. mLSTM is
            characterised by its ability to have different recurrent transition functions
            for each possible input, which we argue makes it more expressive for
            autoregressive density estimation. We demonstrate empirically that mLSTM
            outperforms standard LSTM and its deep variants for a range of character level
            language modelling tasks. In this version of the paper, we regularise mLSTM to
            achieve 1.27 bits/char on text8 and 1.24 bits/char on Hutter Prize. We also
            apply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a
            character level entropy of 1.26 bits/char, corresponding to a word level
            perplexity of 88.8, which is comparable to word level LSTMs regularised in
            similar ways on the same task.
        </summary>
        <author>
            <name>Ben Krause</name>
        </author>
        <author>
            <name>Liang Lu</name>
        </author>
        <author>
            <name>Iain Murray</name>
        </author>
        <author>
            <name>Steve Renals</name>
        </author>
        <link href="http://arxiv.org/abs/1609.07959v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1609.07959v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1610.03098v3</id>
        <updated>2016-10-13T00:37:33Z</updated>
        <published>2016-10-10T21:01:00Z</published>
        <title>Neural Paraphrase Generation with Stacked Residual LSTM Networks</title>
        <summary>  In this paper, we propose a novel neural approach for paraphrase generation.
            Conventional para- phrase generation methods either leverage hand-written rules
            and thesauri-based alignments, or use statistical machine learning principles.
            To the best of our knowledge, this work is the first to explore deep learning
            models for paraphrase generation. Our primary contribution is a stacked
            residual LSTM network, where we add residual connections between LSTM layers.
            This allows for efficient training of deep LSTMs. We evaluate our model and
            other state-of-the-art deep learning models on three different datasets: PPDB,
            WikiAnswers and MSCOCO. Evaluation results demonstrate that our model
            outperforms sequence to sequence, attention-based and bi- directional LSTM
            models on BLEU, METEOR, TER and an embedding-based sentence similarity metric.
        </summary>
        <author>
            <name>Aaditya Prakash</name>
        </author>
        <author>
            <name>Sadid A. Hasan</name>
        </author>
        <author>
            <name>Kathy Lee</name>
        </author>
        <author>
            <name>Vivek Datla</name>
        </author>
        <author>
            <name>Ashequl Qadir</name>
        </author>
        <author>
            <name>Joey Liu</name>
        </author>
        <author>
            <name>Oladimeji Farri</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COLING 2016</arxiv:comment>
        <link href="http://arxiv.org/abs/1610.03098v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1610.03098v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1610.09460v1</id>
        <updated>2016-10-29T06:02:03Z</updated>
        <published>2016-10-29T06:02:03Z</published>
        <title>Building Energy Load Forecasting using Deep Neural Networks</title>
        <summary>  Ensuring sustainability demands more efficient energy management with
            minimized energy wastage. Therefore, the power grid of the future should
            provide an unprecedented level of flexibility in energy management. To that
            end, intelligent decision making requires accurate predictions of future energy
            demand/load, both at aggregate and individual site level. Thus, energy load
            forecasting have received increased attention in the recent past, however has
            proven to be a difficult problem. This paper presents a novel energy load
            forecasting methodology based on Deep Neural Networks, specifically Long Short
            Term Memory (LSTM) algorithms. The presented work investigates two variants of
            the LSTM: 1) standard LSTM and 2) LSTM-based Sequence to Sequence (S2S)
            architecture. Both methods were implemented on a benchmark data set of
            electricity consumption data from one residential customer. Both architectures
            where trained and tested on one hour and one-minute time-step resolution
            datasets. Experimental results showed that the standard LSTM failed at
            one-minute resolution data while performing well in one-hour resolution data.
            It was shown that S2S architecture performed well on both datasets. Further, it
            was shown that the presented methods produced comparable results with the other
            deep learning methods for energy forecasting in literature.
        </summary>
        <author>
            <name>Daniel L. Marino</name>
        </author>
        <author>
            <name>Kasun Amarasinghe</name>
        </author>
        <author>
            <name>Milos Manic</name>
        </author>
        <link href="http://arxiv.org/abs/1610.09460v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1610.09460v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1611.01368v1</id>
        <updated>2016-11-04T13:36:32Z</updated>
        <published>2016-11-04T13:36:32Z</published>
        <title>Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies</title>
        <summary>  The success of long short-term memory (LSTM) neural networks in language
            processing is typically attributed to their ability to capture long-distance
            statistical regularities. Linguistic regularities are often sensitive to
            syntactic structure; can such dependencies be captured by LSTMs, which do not
            have explicit structural representations? We begin addressing this question
            using number agreement in English subject-verb dependencies. We probe the
            architecture's grammatical competence both using training objectives with an
            explicit grammatical target (number prediction, grammaticality judgments) and
            using language models. In the strongly supervised settings, the LSTM achieved
            very high overall accuracy (less than 1% errors), but errors increased when
            sequential and structural information conflicted. The frequency of such errors
            rose sharply in the language-modeling setting. We conclude that LSTMs can
            capture a non-trivial amount of grammatical structure given targeted
            supervision, but stronger architectures may be required to further reduce
            errors; furthermore, the language modeling signal is insufficient for capturing
            syntax-sensitive dependencies, and should be supplemented with more direct
            supervision if such dependencies need to be captured.
        </summary>
        <author>
            <name>Tal Linzen</name>
        </author>
        <author>
            <name>Emmanuel Dupoux</name>
        </author>
        <author>
            <name>Yoav Goldberg</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; to appear in Transactions of the Association for
            Computational Linguistics</arxiv:comment>
        <link href="http://arxiv.org/abs/1611.01368v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1611.01368v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1701.04224v2</id>
        <updated>2017-03-17T14:57:06Z</updated>
        <published>2017-01-16T10:08:22Z</published>
        <title>Auxiliary Multimodal LSTM for Audio-visual Speech Recognition and
            Lipreading</title>
        <summary>  The Aduio-visual Speech Recognition (AVSR) which employs both the video and
            audio information to do Automatic Speech Recognition (ASR) is one of the
            application of multimodal leaning making ASR system more robust and accuracy.
            The traditional models usually treated AVSR as inference or projection but
            strict prior limits its ability. As the revival of deep learning, Deep Neural
            Networks (DNN) becomes an important toolkit in many traditional classification
            tasks including ASR, image classification, natural language processing. Some
            DNN models were used in AVSR like Multimodal Deep Autoencoders (MDAEs),
            Multimodal Deep Belief Network (MDBN) and Multimodal Deep Boltzmann Machine
            (MDBM) that actually work better than traditional methods. However, such DNN
            models have several shortcomings: (1) They don't balance the modal fusion and
            temporal fusion, or even haven't temporal fusion; (2)The architecture of these
            models isn't end-to-end, the training and testing getting cumbersome. We
            propose a DNN model, Auxiliary Multimodal LSTM (am-LSTM), to overcome such
            weakness. The am-LSTM could be trained and tested once, moreover easy to train
            and preventing overfitting automatically. The extensibility and flexibility are
            also take into consideration. The experiments show that am-LSTM is much better
            than traditional methods and other DNN models in three datasets.
        </summary>
        <author>
            <name>Chunlin Tian</name>
        </author>
        <author>
            <name>Weijun Ji</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1701.04224v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1701.04224v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1705.02411v1</id>
        <updated>2017-05-05T22:36:04Z</updated>
        <published>2017-05-05T22:36:04Z</published>
        <title>Max-Pooling Loss Training of Long Short-Term Memory Networks for
            Small-Footprint Keyword Spotting</title>
        <summary>  We propose a max-pooling based loss function for training Long Short-Term
            Memory (LSTM) networks for small-footprint keyword spotting (KWS), with low
            CPU, memory, and latency requirements. The max-pooling loss training can be
            further guided by initializing with a cross-entropy loss trained network. A
            posterior smoothing based evaluation approach is employed to measure keyword
            spotting performance. Our experimental results show that LSTM models trained
            using cross-entropy loss or max-pooling loss outperform a cross-entropy loss
            trained baseline feed-forward Deep Neural Network (DNN). In addition,
            max-pooling loss trained LSTM with randomly initialized network performs better
            compared to cross-entropy loss trained LSTM. Finally, the max-pooling loss
            trained LSTM initialized with a cross-entropy pre-trained network shows the
            best performance, which yields $67.6\%$ relative reduction compared to baseline
            feed-forward DNN in Area Under the Curve (AUC) measure.
        </summary>
        <author>
            <name>Ming Sun</name>
        </author>
        <author>
            <name>Anirudh Raju</name>
        </author>
        <author>
            <name>George Tucker</name>
        </author>
        <author>
            <name>Sankaran Panchapagesan</name>
        </author>
        <author>
            <name>Gengshen Fu</name>
        </author>
        <author>
            <name>Arindam Mandal</name>
        </author>
        <author>
            <name>Spyros Matsoukas</name>
        </author>
        <author>
            <name>Nikko Strom</name>
        </author>
        <author>
            <name>Shiv Vitaladevuni</name>
        </author>
        <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SLT.2016.7846306</arxiv:doi>
        <link title="doi" href="http://dx.doi.org/10.1109/SLT.2016.7846306" rel="related"/>
        <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Spoken Language Technology Workshop (SLT), 2016 IEEE (pp.
            474-480). IEEE</arxiv:journal_ref>
        <link href="http://arxiv.org/abs/1705.02411v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1705.02411v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1705.06861v1</id>
        <updated>2017-05-19T05:14:31Z</updated>
        <published>2017-05-19T05:14:31Z</published>
        <title>Prediction of Sea Surface Temperature using Long Short-Term Memory</title>
        <summary>  This letter adopts long short-term memory(LSTM) to predict sea surface
            temperature(SST), which is the first attempt, to our knowledge, to use
            recurrent neural network to solve the problem of SST prediction, and to make
            one week and one month daily prediction. We formulate the SST prediction
            problem as a time series regression problem. LSTM is a special kind of
            recurrent neural network, which introduces gate mechanism into vanilla RNN to
            prevent the vanished or exploding gradient problem. It has strong ability to
            model the temporal relationship of time series data and can handle the
            long-term dependency problem well. The proposed network architecture is
            composed of two kinds of layers: LSTM layer and full-connected dense layer.
            LSTM layer is utilized to model the time series relationship. Full-connected
            layer is utilized to map the output of LSTM layer to a final prediction. We
            explore the optimal setting of this architecture by experiments and report the
            accuracy of coastal seas of China to confirm the effectiveness of the proposed
            method. In addition, we also show its online updated characteristics.
        </summary>
        <author>
            <name>Qin Zhang</name>
        </author>
        <author>
            <name>Hui Wang</name>
        </author>
        <author>
            <name>Junyu Dong</name>
        </author>
        <author>
            <name>Guoqiang Zhong</name>
        </author>
        <author>
            <name>Xin Sun</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 page, 5 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1705.06861v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1705.06861v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1707.06226v1</id>
        <updated>2017-07-19T01:21:26Z</updated>
        <published>2017-07-19T01:21:26Z</published>
        <title>The Role of Conversation Context for Sarcasm Detection in Online
            Interactions</title>
        <summary>  Computational models for sarcasm detection have often relied on the content
            of utterances in isolation. However, speaker's sarcastic intent is not always
            obvious without additional context. Focusing on social media discussions, we
            investigate two issues: (1) does modeling of conversation context help in
            sarcasm detection and (2) can we understand what part of conversation context
            triggered the sarcastic reply. To address the first issue, we investigate
            several types of Long Short-Term Memory (LSTM) networks that can model both the
            conversation context and the sarcastic response. We show that the conditional
            LSTM network (Rocktaschel et al., 2015) and LSTM networks with sentence level
            attention on context and response outperform the LSTM model that reads only the
            response. To address the second issue, we present a qualitative analysis of
            attention weights produced by the LSTM models with attention and discuss the
            results compared with human performance on the task.
        </summary>
        <author>
            <name>Debanjan Ghosh</name>
        </author>
        <author>
            <name>Alexander Richard Fabbri</name>
        </author>
        <author>
            <name>Smaranda Muresan</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGDial 2017</arxiv:comment>
        <link href="http://arxiv.org/abs/1707.06226v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1707.06226v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1708.05682v1</id>
        <updated>2017-08-17T01:37:21Z</updated>
        <published>2017-08-17T01:37:21Z</published>
        <title>An Improved Residual LSTM Architecture for Acoustic Modeling</title>
        <summary>  Long Short-Term Memory (LSTM) is the primary recurrent neural networks
            architecture for acoustic modeling in automatic speech recognition systems.
            Residual learning is an efficient method to help neural networks converge
            easier and faster. In this paper, we propose several types of residual LSTM
            methods for our acoustic modeling. Our experiments indicate that, compared with
            classic LSTM, our architecture shows more than 8% relative reduction in Phone
            Error Rate (PER) on TIMIT tasks. At the same time, our residual fast LSTM
            approach shows 4% relative reduction in PER on the same task. Besides, we find
            that all this architecture could have good results on THCHS-30, Librispeech and
            Switchboard corpora.
        </summary>
        <author>
            <name>Lu Huang</name>
        </author>
        <author>
            <name>Jiasong Sun</name>
        </author>
        <author>
            <name>Ji Xu</name>
        </author>
        <author>
            <name>Yi Yang</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1708.05682v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1708.05682v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1708.08959v2</id>
        <updated>2017-09-08T22:46:59Z</updated>
        <published>2017-08-29T18:25:35Z</published>
        <title>A Simple LSTM model for Transition-based Dependency Parsing</title>
        <summary>  We present a simple LSTM-based transition-based dependency parser. Our model
            is composed of a single LSTM hidden layer replacing the hidden layer in the
            usual feed-forward network architecture. We also propose a new initialization
            method that uses the pre-trained weights from a feed-forward neural network to
            initialize our LSTM-based model. We also show that using dropout on the input
            layer has a positive effect on performance. Our final parser achieves a 93.06%
            unlabeled and 91.01% labeled attachment score on the Penn Treebank. We
            additionally replace LSTMs with GRUs and Elman units in our model and explore
            the effectiveness of our initialization method on individual gates constituting
            all three types of RNN units.
        </summary>
        <author>
            <name>Mohab Elkaref</name>
        </author>
        <author>
            <name>Bernd Bohnet</name>
        </author>
        <link href="http://arxiv.org/abs/1708.08959v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1708.08959v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1710.01693v1</id>
        <updated>2017-09-29T20:36:02Z</updated>
        <published>2017-09-29T20:36:02Z</published>
        <title>Model-free prediction of noisy chaotic time series by deep learning</title>
        <summary>  We present a deep neural network for a model-free prediction of a chaotic
            dynamical system from noisy observations. The proposed deep learning model aims
            to predict the conditional probability distribution of a state variable. The
            Long Short-Term Memory network (LSTM) is employed to model the nonlinear
            dynamics and a softmax layer is used to approximate a probability distribution.
            The LSTM model is trained by minimizing a regularized cross-entropy function.
            The LSTM model is validated against delay-time chaotic dynamical systems,
            Mackey-Glass and Ikeda equations. It is shown that the present LSTM makes a
            good prediction of the nonlinear dynamics by effectively filtering out the
            noise. It is found that the prediction uncertainty of a multiple-step forecast
            of the LSTM model is not a monotonic function of time; the predicted standard
            deviation may increase or decrease dynamically in time.
        </summary>
        <author>
            <name>Kyongmin Yeo</name>
        </author>
        <link href="http://arxiv.org/abs/1710.01693v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1710.01693v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
        <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1508.00657v2</id>
        <updated>2015-08-11T17:33:47Z</updated>
        <published>2015-08-04T04:36:36Z</published>
        <title>Improved Transition-Based Parsing by Modeling Characters instead of
            Words with LSTMs</title>
        <summary>  We present extensions to a continuous-state dependency parsing method that
            makes it applicable to morphologically rich languages. Starting with a
            high-performance transition-based parser that uses long short-term memory
            (LSTM) recurrent neural networks to learn representations of the parser state,
            we replace lookup-based word representations with representations constructed
            from the orthographic representations of the words, also using LSTMs. This
            allows statistical sharing across word forms that are similar on the surface.
            Experiments for morphologically rich languages show that the parsing model
            benefits from incorporating the character-based encodings of words.
        </summary>
        <author>
            <name>Miguel Ballesteros</name>
        </author>
        <author>
            <name>Chris Dyer</name>
        </author>
        <author>
            <name>Noah A. Smith</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of EMNLP 2015</arxiv:comment>
        <link href="http://arxiv.org/abs/1508.00657v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1508.00657v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1409.2329v5</id>
        <updated>2015-02-19T14:46:00Z</updated>
        <published>2014-09-08T13:08:00Z</published>
        <title>Recurrent Neural Network Regularization</title>
        <summary>  We present a simple regularization technique for Recurrent Neural Networks
            (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful
            technique for regularizing neural networks, does not work well with RNNs and
            LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show
            that it substantially reduces overfitting on a variety of tasks. These tasks
            include language modeling, speech recognition, image caption generation, and
            machine translation.
        </summary>
        <author>
            <name>Wojciech Zaremba</name>
        </author>
        <author>
            <name>Ilya Sutskever</name>
        </author>
        <author>
            <name>Oriol Vinyals</name>
        </author>
        <link href="http://arxiv.org/abs/1409.2329v5" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1409.2329v5" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1603.09457v1</id>
        <updated>2016-03-31T05:14:10Z</updated>
        <published>2016-03-31T05:14:10Z</published>
        <title>LSTM based Conversation Models</title>
        <summary>  In this paper, we present a conversational model that incorporates both
            context and participant role for two-party conversations. Different
            architectures are explored for integrating participant role and context
            information into a Long Short-term Memory (LSTM) language model. The
            conversational model can function as a language model or a language generation
            model. Experiments on the Ubuntu Dialog Corpus show that our model can capture
            multiple turn interaction between participants. The proposed method outperforms
            a traditional LSTM model as measured by language model perplexity and response
            ranking. Generated responses show characteristic differences between the two
            participant roles.
        </summary>
        <author>
            <name>Yi Luan</name>
        </author>
        <author>
            <name>Yangfeng Ji</name>
        </author>
        <author>
            <name>Mari Ostendorf</name>
        </author>
        <link href="http://arxiv.org/abs/1603.09457v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1603.09457v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1610.03167v1</id>
        <updated>2016-10-11T03:02:38Z</updated>
        <published>2016-10-11T03:02:38Z</published>
        <title>An Empirical Exploration of Skip Connections for Sequential Tagging</title>
        <summary>  In this paper, we empirically explore the effects of various kinds of skip
            connections in stacked bidirectional LSTMs for sequential tagging. We
            investigate three kinds of skip connections connecting to LSTM cells: (a) skip
            connections to the gates, (b) skip connections to the internal states and (c)
            skip connections to the cell outputs. We present comprehensive experiments
            showing that skip connections to cell outputs outperform the remaining two.
            Furthermore, we observe that using gated identity functions as skip mappings
            works pretty well. Based on this novel skip connections, we successfully train
            deep stacked bidirectional LSTM models and obtain state-of-the-art results on
            CCG supertagging and comparable results on POS tagging.
        </summary>
        <author>
            <name>Huijia Wu</name>
        </author>
        <author>
            <name>Jiajun Zhang</name>
        </author>
        <author>
            <name>Chengqing Zong</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at COLING 2016</arxiv:comment>
        <link href="http://arxiv.org/abs/1610.03167v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1610.03167v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1705.00364v1</id>
        <updated>2017-04-30T19:18:22Z</updated>
        <published>2017-04-30T19:18:22Z</published>
        <title>Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings</title>
        <summary>  We consider the problem of learning general-purpose, paraphrastic sentence
            embeddings, revisiting the setting of Wieting et al. (2016b). While they found
            LSTM recurrent networks to underperform word averaging, we present several
            developments that together produce the opposite conclusion. These include
            training on sentence pairs rather than phrase pairs, averaging states to
            represent sequences, and regularizing aggressively. These improve LSTMs in both
            transfer learning and supervised settings. We also introduce a new recurrent
            architecture, the Gated Recurrent Averaging Network, that is inspired by
            averaging and LSTMs while outperforming them both. We analyze our learned
            models, finding evidence of preferences for particular parts of speech and
            dependency relations.
        </summary>
        <author>
            <name>John Wieting</name>
        </author>
        <author>
            <name>Kevin Gimpel</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a long paper at ACL 2017</arxiv:comment>
        <link href="http://arxiv.org/abs/1705.00364v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1705.00364v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1709.08011v1</id>
        <updated>2017-09-23T06:15:37Z</updated>
        <published>2017-09-23T06:15:37Z</published>
        <title>Long Short-Term Memory for Japanese Word Segmentation</title>
        <summary>  This study presents a Long Short-Term Memory (LSTM) neural network approach
            to Japanese word segmentation (JWS). Previous studies on Chinese word
            segmentation (CWS) succeeded in using recurrent neural networks such as LSTM
            and gated recurrent units (GRU). However, in contrast to Chinese, Japanese
            includes several character types, such as hiragana, katakana, and kanji, that
            produce orthographic variations and increase the difficulty of word
            segmentation. Additionally, it is important for JWS tasks to consider a global
            context, and yet traditional JWS approaches rely on local features. In order to
            address this problem, this study proposes employing an LSTM-based approach to
            JWS. The experimental results indicate that the proposed model achieves
            state-of-the-art accuracy with respect to various Japanese corpora.
        </summary>
        <author>
            <name>Yoshiaki Kitagawa</name>
        </author>
        <author>
            <name>Mamoru Komachi</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1709.08011v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1709.08011v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1511.04510v1</id>
        <updated>2015-11-14T05:42:50Z</updated>
        <published>2015-11-14T05:42:50Z</published>
        <title>Semantic Object Parsing with Local-Global Long Short-Term Memory</title>
        <summary>  Semantic object parsing is a fundamental task for understanding objects in
            detail in computer vision community, where incorporating multi-level contextual
            information is critical for achieving such fine-grained pixel-level
            recognition. Prior methods often leverage the contextual information through
            post-processing predicted confidence maps. In this work, we propose a novel
            deep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlessly
            incorporate short-distance and long-distance spatial dependencies into the
            feature learning over all pixel positions. In each LG-LSTM layer, local
            guidance from neighboring positions and global guidance from the whole image
            are imposed on each position to better exploit complex local and global
            contextual information. Individual LSTMs for distinct spatial dimensions are
            also utilized to intrinsically capture various spatial layouts of semantic
            parts in the images, yielding distinct hidden and memory cells of each position
            for each dimension. In our parsing approach, several LG-LSTM layers are stacked
            and appended to the intermediate convolutional layers to directly enhance
            visual features, allowing network parameters to be learned in an end-to-end
            way. The long chains of sequential computation by stacked LG-LSTM layers also
            enable each pixel to sense a much larger region for inference benefiting from
            the memorization of previous dependencies in all positions along all
            dimensions. Comprehensive evaluations on three public datasets well demonstrate
            the significant superiority of our LG-LSTM over other state-of-the-art methods.
        </summary>
        <author>
            <name>Xiaodan Liang</name>
        </author>
        <author>
            <name>Xiaohui Shen</name>
        </author>
        <author>
            <name>Donglai Xiang</name>
        </author>
        <author>
            <name>Jiashi Feng</name>
        </author>
        <author>
            <name>Liang Lin</name>
        </author>
        <author>
            <name>Shuicheng Yan</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1511.04510v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1511.04510v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1612.08534v1</id>
        <updated>2016-12-27T08:36:48Z</updated>
        <published>2016-12-27T08:36:48Z</published>
        <title>Robust LSTM-Autoencoders for Face De-Occlusion in the Wild</title>
        <summary>  Face recognition techniques have been developed significantly in recent
            years. However, recognizing faces with partial occlusion is still challenging
            for existing face recognizers which is heavily desired in real-world
            applications concerning surveillance and security. Although much research
            effort has been devoted to developing face de-occlusion methods, most of them
            can only work well under constrained conditions, such as all the faces are from
            a pre-defined closed set. In this paper, we propose a robust LSTM-Autoencoders
            (RLA) model to effectively restore partially occluded faces even in the wild.
            The RLA model consists of two LSTM components, which aims at occlusion-robust
            face encoding and recurrent occlusion removal respectively. The first one,
            named multi-scale spatial LSTM encoder, reads facial patches of various scales
            sequentially to output a latent representation, and occlusion-robustness is
            achieved owing to the fact that the influence of occlusion is only upon some of
            the patches. Receiving the representation learned by the encoder, the LSTM
            decoder with a dual channel architecture reconstructs the overall face and
            detects occlusion simultaneously, and by feat of LSTM, the decoder breaks down
            the task of face de-occlusion into restoring the occluded part step by step.
            Moreover, to minimize identify information loss and guarantee face recognition
            accuracy over recovered faces, we introduce an identity-preserving adversarial
            training scheme to further improve RLA. Extensive experiments on both synthetic
            and real datasets of faces with occlusion clearly demonstrate the effectiveness
            of our proposed RLA in removing different types of facial occlusion at various
            locations. The proposed method also provides significantly larger performance
            gain than other de-occlusion methods in promoting recognition performance over
            partially-occluded faces.
        </summary>
        <author>
            <name>Fang Zhao</name>
        </author>
        <author>
            <name>Jiashi Feng</name>
        </author>
        <author>
            <name>Jian Zhao</name>
        </author>
        <author>
            <name>Wenhan Yang</name>
        </author>
        <author>
            <name>Shuicheng Yan</name>
        </author>
        <link href="http://arxiv.org/abs/1612.08534v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1612.08534v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1703.07090v1</id>
        <updated>2017-03-21T08:24:50Z</updated>
        <published>2017-03-21T08:24:50Z</published>
        <title>Deep LSTM for Large Vocabulary Continuous Speech Recognition</title>
        <summary>  Recurrent neural networks (RNNs), especially long short-term memory (LSTM)
            RNNs, are effective network for sequential task like speech recognition. Deeper
            LSTM models perform well on large vocabulary continuous speech recognition,
            because of their impressive learning ability. However, it is more difficult to
            train a deeper network. We introduce a training framework with layer-wise
            training and exponential moving average methods for deeper LSTM models. It is a
            competitive framework that LSTM models of more than 7 layers are successfully
            trained on Shenma voice search data in Mandarin and they outperform the deep
            LSTM models trained by conventional approach. Moreover, in order for online
            streaming speech recognition applications, the shallow model with low real time
            factor is distilled from the very deep model. The recognition accuracy have
            little loss in the distillation process. Therefore, the model trained with the
            proposed training framework reduces relative 14\% character error rate,
            compared to original model which has the similar real-time capability.
            Furthermore, the novel transfer learning strategy with segmental Minimum
            Bayes-Risk is also introduced in the framework. The strategy makes it possible
            that training with only a small part of dataset could outperform full dataset
            training from the beginning.
        </summary>
        <author>
            <name>Xu Tian</name>
        </author>
        <author>
            <name>Jun Zhang</name>
        </author>
        <author>
            <name>Zejun Ma</name>
        </author>
        <author>
            <name>Yi He</name>
        </author>
        <author>
            <name>Juan Wei</name>
        </author>
        <author>
            <name>Peihao Wu</name>
        </author>
        <author>
            <name>Wenchang Situ</name>
        </author>
        <author>
            <name>Shuai Li</name>
        </author>
        <author>
            <name>Yang Zhang</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. arXiv admin note: text overlap with arXiv:1703.01024</arxiv:comment>
        <link href="http://arxiv.org/abs/1703.07090v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1703.07090v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1707.05740v3</id>
        <updated>2017-08-22T02:36:41Z</updated>
        <published>2017-07-18T17:03:53Z</published>
        <title>Skeleton Based Human Action Recognition with Global Context-Aware
            Attention LSTM Networks</title>
        <summary>  Human action recognition in 3D skeleton sequences has attracted a lot of
            research attention. Recently, Long Short-Term Memory (LSTM) networks have shown
            promising performance in this task due to their strengths in modeling the
            dependencies and dynamics in sequential data. As not all skeletal joints are
            informative for action recognition, and the irrelevant joints often bring noise
            which can degrade the performance, we need to pay more attention to the
            informative ones. However, the original LSTM network does not have explicit
            attention ability. In this paper, we propose a new class of LSTM network,
            Global Context-Aware Attention LSTM (GCA-LSTM), for skeleton based action
            recognition. This network is capable of selectively focusing on the informative
            joints in each frame of each skeleton sequence by using a global context memory
            cell. To further improve the attention capability of our network, we also
            introduce a recurrent attention mechanism, with which the attention performance
            of the network can be enhanced progressively. Moreover, we propose a stepwise
            training scheme in order to train our network effectively. Our approach
            achieves state-of-the-art performance on five challenging benchmark datasets
            for skeleton based action recognition.
        </summary>
        <author>
            <name>Jun Liu</name>
        </author>
        <author>
            <name>Gang Wang</name>
        </author>
        <author>
            <name>Ling-Yu Duan</name>
        </author>
        <author>
            <name>Ping Hu</name>
        </author>
        <author>
            <name>Alex C. Kot</name>
        </author>
        <link href="http://arxiv.org/abs/1707.05740v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1707.05740v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1709.05038v1</id>
        <updated>2017-09-15T02:53:16Z</updated>
        <published>2017-09-15T02:53:16Z</published>
        <title>Self-Guiding Multimodal LSTM - when we do not have a perfect training
            dataset for image captioning</title>
        <summary>  In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning
            model is proposed to handle uncontrolled imbalanced real-world image-sentence
            dataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165
            images and the original text descriptions uploaded by the users are utilized as
            the ground truth for training. Descriptions in FlickrNYC dataset vary
            dramatically ranging from short term-descriptions to long
            paragraph-descriptions and can describe any visual aspects, or even refer to
            objects that are not depicted. To deal with the imbalanced and noisy situation
            and to fully explore the dataset itself, we propose a novel guiding textual
            feature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of
            m-LSTM is based on the portion of data in which the image content and the
            corresponding descriptions are strongly bonded. Afterwards, during the training
            of sg-LSTM on the rest training data, this guiding information serves as
            additional input to the network along with the image representations and the
            ground-truth descriptions. By integrating these input components into a
            multimodal block, we aim to form a training scheme with the textual information
            tightly coupled with the image content. The experimental results demonstrate
            that the proposed sg-LSTM model outperforms the traditional state-of-the-art
            multimodal RNN captioning framework in successfully describing the key
            components of the input images.
        </summary>
        <author>
            <name>Yang Xian</name>
        </author>
        <author>
            <name>Yingli Tian</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is under consideration at Computer Vision and Image
            Understanding</arxiv:comment>
        <link href="http://arxiv.org/abs/1709.05038v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1709.05038v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1412.3191v2</id>
        <updated>2014-12-14T03:18:33Z</updated>
        <published>2014-12-10T04:06:38Z</published>
        <title>Bach in 2014: Music Composition with Recurrent Neural Network</title>
        <summary>  We propose a framework for computer music composition that uses resilient
            propagation (RProp) and long short term memory (LSTM) recurrent neural network.
            In this paper, we show that LSTM network learns the structure and
            characteristics of music pieces properly by demonstrating its ability to
            recreate music. We also show that predicting existing music using RProp
            outperforms Back propagation through time (BPTT).
        </summary>
        <author>
            <name>I-Ting Liu</name>
        </author>
        <author>
            <name>Bhiksha Ramakrishnan</name>
        </author>
        <link href="http://arxiv.org/abs/1412.3191v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1412.3191v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1510.07641v2</id>
        <updated>2017-03-21T21:28:55Z</updated>
        <published>2015-10-26T20:18:56Z</published>
        <title>Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks</title>
        <summary>  We present a novel application of LSTM recurrent neural networks to
            multilabel classification of diagnoses given variable-length time series of
            clinical measurements. Our method outperforms a strong baseline on a variety of
            metrics.
        </summary>
        <author>
            <name>Zachary C. Lipton</name>
        </author>
        <author>
            <name>David C. Kale</name>
        </author>
        <author>
            <name>Randall C. Wetzel</name>
        </author>
        <link href="http://arxiv.org/abs/1510.07641v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1510.07641v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1707.07755v2</id>
        <updated>2017-08-02T15:47:32Z</updated>
        <published>2017-07-24T21:33:21Z</published>
        <title>AMR Parsing using Stack-LSTMs</title>
        <summary>  We present a transition-based AMR parser that directly generates AMR parses
            from plain text. We use Stack-LSTMs to represent our parser state and make
            decisions greedily. In our experiments, we show that our parser achieves very
            competitive scores on English using only AMR training data. Adding additional
            information, such as POS tags and dependency trees, improves the results
            further.
        </summary>
        <author>
            <name>Miguel Ballesteros</name>
        </author>
        <author>
            <name>Yaser Al-Onaizan</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2017</arxiv:comment>
        <link href="http://arxiv.org/abs/1707.07755v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1707.07755v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1410.4615v3</id>
        <updated>2015-02-19T15:33:35Z</updated>
        <published>2014-10-17T01:35:12Z</published>
        <title>Learning to Execute</title>
        <summary>  Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are
            widely used because they are expressive and are easy to train. Our interest
            lies in empirically evaluating the expressiveness and the learnability of LSTMs
            in the sequence-to-sequence regime by training them to evaluate short computer
            programs, a domain that has traditionally been seen as too complex for neural
            networks. We consider a simple class of programs that can be evaluated with a
            single left-to-right pass using constant memory. Our main result is that LSTMs
            can learn to map the character-level representations of such programs to their
            correct outputs. Notably, it was necessary to use curriculum learning, and
            while conventional curriculum learning proved ineffective, we developed a new
            variant of curriculum learning that improved our networks' performance in all
            experimental conditions. The improved curriculum had a dramatic impact on an
            addition problem, making it possible to train an LSTM to add two 9-digit
            numbers with 99% accuracy.
        </summary>
        <author>
            <name>Wojciech Zaremba</name>
        </author>
        <author>
            <name>Ilya Sutskever</name>
        </author>
        <link href="http://arxiv.org/abs/1410.4615v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1410.4615v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1508.01745v2</id>
        <updated>2015-08-26T17:16:25Z</updated>
        <published>2015-08-07T16:16:44Z</published>
        <title>Semantically Conditioned LSTM-based Natural Language Generation for
            Spoken Dialogue Systems</title>
        <summary>  Natural language generation (NLG) is a critical component of spoken dialogue
            and it has a significant impact both on usability and perceived quality. Most
            NLG systems in common use employ rules and heuristics and tend to generate
            rigid and stylised responses without the natural variation of human language.
            They are also not easily scaled to systems covering multiple domains and
            languages. This paper presents a statistical language generator based on a
            semantically controlled Long Short-term Memory (LSTM) structure. The LSTM
            generator can learn from unaligned data by jointly optimising sentence planning
            and surface realisation using a simple cross entropy training criterion, and
            language variation can be easily achieved by sampling from output candidates.
            With fewer heuristics, an objective evaluation in two differing test domains
            showed the proposed method improved performance compared to previous methods.
            Human judges scored the LSTM system higher on informativeness and naturalness
            and overall preferred it to the other systems.
        </summary>
        <author>
            <name>Tsung-Hsien Wen</name>
        </author>
        <author>
            <name>Milica Gasic</name>
        </author>
        <author>
            <name>Nikola Mrksic</name>
        </author>
        <author>
            <name>Pei-Hao Su</name>
        </author>
        <author>
            <name>David Vandyke</name>
        </author>
        <author>
            <name>Steve Young</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be appear in EMNLP 2015</arxiv:comment>
        <link href="http://arxiv.org/abs/1508.01745v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1508.01745v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1402.1128v1</id>
        <updated>2014-02-05T19:01:51Z</updated>
        <published>2014-02-05T19:01:51Z</published>
        <title>Long Short-Term Memory Based Recurrent Neural Network Architectures for
            Large Vocabulary Speech Recognition</title>
        <summary>  Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)
            architecture that has been designed to address the vanishing and exploding
            gradient problems of conventional RNNs. Unlike feedforward neural networks,
            RNNs have cyclic connections making them powerful for modeling sequences. They
            have been successfully used for sequence labeling and sequence prediction
            tasks, such as handwriting recognition, language modeling, phonetic labeling of
            acoustic frames. However, in contrast to the deep neural networks, the use of
            RNNs in speech recognition has been limited to phone recognition in small scale
            tasks. In this paper, we present novel LSTM based RNN architectures which make
            more effective use of model parameters to train acoustic models for large
            vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at
            various numbers of parameters and configurations. We show that LSTM models
            converge quickly and give state of the art speech recognition performance for
            relatively small sized models.
        </summary>
        <author>
            <name>Haim Sak</name>
        </author>
        <author>
            <name>Andrew Senior</name>
        </author>
        <author>
            <name>Franoise Beaufays</name>
        </author>
        <link href="http://arxiv.org/abs/1402.1128v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1402.1128v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1507.06947v1</id>
        <updated>2015-07-24T18:28:32Z</updated>
        <published>2015-07-24T18:28:32Z</published>
        <title>Fast and Accurate Recurrent Neural Network Acoustic Models for Speech
            Recognition</title>
        <summary>  We have recently shown that deep Long Short-Term Memory (LSTM) recurrent
            neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as
            acoustic models for speech recognition. More recently, we have shown that the
            performance of sequence trained context dependent (CD) hidden Markov model
            (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained
            phone models initialized with connectionist temporal classification (CTC). In
            this paper, we present techniques that further improve performance of LSTM RNN
            acoustic models for large vocabulary speech recognition. We show that frame
            stacking and reduced frame rate lead to more accurate models and faster
            decoding. CD phone modeling leads to further improvements. We also present
            initial results for LSTM RNN models outputting words directly.
        </summary>
        <author>
            <name>Haim Sak</name>
        </author>
        <author>
            <name>Andrew Senior</name>
        </author>
        <author>
            <name>Kanishka Rao</name>
        </author>
        <author>
            <name>Franoise Beaufays</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the INTERSPEECH 2015 proceedings</arxiv:comment>
        <link href="http://arxiv.org/abs/1507.06947v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1507.06947v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1510.08983v2</id>
        <updated>2016-01-11T09:48:01Z</updated>
        <published>2015-10-30T06:40:14Z</published>
        <title>Highway Long Short-Term Memory RNNs for Distant Speech Recognition</title>
        <summary>  In this paper, we extend the deep long short-term memory (DLSTM) recurrent
            neural networks by introducing gated direct connections between memory cells in
            adjacent layers. These direct links, called highway connections, enable
            unimpeded information flow across different layers and thus alleviate the
            gradient vanishing problem when building deeper LSTMs. We further introduce the
            latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole
            history while keeping the latency under control. Efficient algorithms are
            proposed to train these novel networks using both frame and sequence
            discriminative criteria. Experiments on the AMI distant speech recognition
            (DSR) task indicate that we can train deeper LSTMs and achieve better
            improvement from sequence training with highway LSTMs (HLSTMs). Our novel model
            obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all
            previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and
            $5.3\%$ relative improvement respectively.
        </summary>
        <author>
            <name>Yu Zhang</name>
        </author>
        <author>
            <name>Guoguo Chen</name>
        </author>
        <author>
            <name>Dong Yu</name>
        </author>
        <author>
            <name>Kaisheng Yao</name>
        </author>
        <author>
            <name>Sanjeev Khudanpur</name>
        </author>
        <author>
            <name>James Glass</name>
        </author>
        <link href="http://arxiv.org/abs/1510.08983v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1510.08983v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1511.06433v3</id>
        <updated>2016-09-14T14:36:53Z</updated>
        <published>2015-11-19T22:48:59Z</published>
        <title>Blending LSTMs into CNNs</title>
        <summary>  We consider whether deep convolutional networks (CNNs) can represent decision
            functions with similar accuracy as recurrent networks such as LSTMs. First, we
            show that a deep CNN with an architecture inspired by the models recently
            introduced in image recognition can yield better accuracy than previous
            convolutional and LSTM networks on the standard 309h Switchboard automatic
            speech recognition task. Then we show that even more accurate CNNs can be
            trained under the guidance of LSTMs using a variant of model compression, which
            we call model blending because the teacher and student models are similar in
            complexity but different in inductive bias. Blending further improves the
            accuracy of our CNN, yielding a computationally efficient model of accuracy
            higher than any of the other individual models. Examining the effect of "dark
            knowledge" in this model compression task, we find that less than 1% of the
            highest probability labels are needed for accurate model compression.
        </summary>
        <author>
            <name>Krzysztof J. Geras</name>
        </author>
        <author>
            <name>Abdel-rahman Mohamed</name>
        </author>
        <author>
            <name>Rich Caruana</name>
        </author>
        <author>
            <name>Gregor Urban</name>
        </author>
        <author>
            <name>Shengjie Wang</name>
        </author>
        <author>
            <name>Ozlem Aslan</name>
        </author>
        <author>
            <name>Matthai Philipose</name>
        </author>
        <author>
            <name>Matthew Richardson</name>
        </author>
        <author>
            <name>Charles Sutton</name>
        </author>
        <link href="http://arxiv.org/abs/1511.06433v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1511.06433v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1601.00770v3</id>
        <updated>2016-06-08T01:08:08Z</updated>
        <published>2016-01-05T08:53:05Z</published>
        <title>End-to-End Relation Extraction using LSTMs on Sequences and Tree
            Structures</title>
        <summary>  We present a novel end-to-end neural model to extract entities and relations
            between them. Our recurrent neural network based model captures both word
            sequence and dependency tree substructure information by stacking bidirectional
            tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows
            our model to jointly represent both entities and relations with shared
            parameters in a single model. We further encourage detection of entities during
            training and use of entity information in relation extraction via entity
            pretraining and scheduled sampling. Our model improves over the
            state-of-the-art feature-based model on end-to-end relation extraction,
            achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and
            ACE2004, respectively. We also show that our LSTM-RNN based model compares
            favorably to the state-of-the-art CNN based model (in F1-score) on nominal
            relation classification (SemEval-2010 Task 8). Finally, we present an extensive
            ablation analysis of several model components.
        </summary>
        <author>
            <name>Makoto Miwa</name>
        </author>
        <author>
            <name>Mohit Bansal</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the Association for Computational
            Linguistics (ACL), 2016. 13 pages, 1 figure, 6 tables</arxiv:comment>
        <link href="http://arxiv.org/abs/1601.00770v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1601.00770v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1601.02539v1</id>
        <updated>2016-01-11T17:54:53Z</updated>
        <published>2016-01-11T17:54:53Z</published>
        <title>Investigating gated recurrent neural networks for speech synthesis</title>
        <summary>  Recently, recurrent neural networks (RNNs) as powerful sequence models have
            re-emerged as a potential acoustic model for statistical parametric speech
            synthesis (SPSS). The long short-term memory (LSTM) architecture is
            particularly attractive because it addresses the vanishing gradient problem in
            standard RNNs, making them easier to train. Although recent studies have
            demonstrated that LSTMs can achieve significantly better performance on SPSS
            than deep feed-forward neural networks, little is known about why. Here we
            attempt to answer two questions: a) why do LSTMs work well as a sequence model
            for SPSS; b) which component (e.g., input gate, output gate, forget gate) is
            most important. We present a visual analysis alongside a series of experiments,
            resulting in a proposal for a simplified architecture. The simplified
            architecture has significantly fewer parameters than an LSTM, thus reducing
            generation complexity considerably without degrading quality.
        </summary>
        <author>
            <name>Zhizheng Wu</name>
        </author>
        <author>
            <name>Simon King</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICASSP 2016</arxiv:comment>
        <link href="http://arxiv.org/abs/1601.02539v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1601.02539v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1602.06291v2</id>
        <updated>2016-05-31T17:19:09Z</updated>
        <published>2016-02-19T20:52:08Z</published>
        <title>Contextual LSTM (CLSTM) models for Large scale NLP tasks</title>
        <summary>  Documents exhibit sequential structure at multiple levels of abstraction
            (e.g., sentences, paragraphs, sections). These abstractions constitute a
            natural hierarchy for representing the context in which to infer the meaning of
            words and larger fragments of text. In this paper, we present CLSTM (Contextual
            LSTM), an extension of the recurrent neural network LSTM (Long-Short Term
            Memory) model, where we incorporate contextual features (e.g., topics) into the
            model. We evaluate CLSTM on three specific NLP tasks: word prediction, next
            sentence selection, and sentence topic prediction. Results from experiments run
            on two corpora, English documents in Wikipedia and a subset of articles from a
            recent snapshot of English Google News, indicate that using both words and
            topics as features improves performance of the CLSTM models over baseline LSTM
            models for these tasks. For example on the next sentence selection task, we get
            relative accuracy improvements of 21% for the Wikipedia dataset and 18% for the
            Google News dataset. This clearly demonstrates the significant benefit of using
            context appropriately in natural language (NL) tasks. This has implications for
            a wide variety of NL applications like question answering, sentence completion,
            paraphrase generation, and next utterance prediction in dialog systems.
        </summary>
        <author>
            <name>Shalini Ghosh</name>
        </author>
        <author>
            <name>Oriol Vinyals</name>
        </author>
        <author>
            <name>Brian Strope</name>
        </author>
        <author>
            <name>Scott Roy</name>
        </author>
        <author>
            <name>Tom Dean</name>
        </author>
        <author>
            <name>Larry Heck</name>
        </author>
        <link href="http://arxiv.org/abs/1602.06291v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1602.06291v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1604.01931v2</id>
        <updated>2016-04-08T06:21:57Z</updated>
        <published>2016-04-07T09:20:51Z</published>
        <title>Geometric Scene Parsing with Hierarchical LSTM</title>
        <summary>  This paper addresses the problem of geometric scene parsing, i.e.
            simultaneously labeling geometric surfaces (e.g. sky, ground and vertical
            plane) and determining the interaction relations (e.g. layering, supporting,
            siding and affinity) between main regions. This problem is more challenging
            than the traditional semantic scene labeling, as recovering geometric
            structures necessarily requires the rich and diverse contextual information. To
            achieve these goals, we propose a novel recurrent neural network model, named
            Hierarchical Long Short-Term Memory (H-LSTM). It contains two coupled
            sub-networks: the Pixel LSTM (P-LSTM) and the Multi-scale Super-pixel LSTM
            (MS-LSTM) for handling the surface labeling and relation prediction,
            respectively. The two sub-networks provide complementary information to each
            other to exploit hierarchical scene contexts, and they are jointly optimized
            for boosting the performance. Our extensive experiments show that our model is
            capable of parsing scene geometric structures and outperforming several
            state-of-the-art methods by large margins. In addition, we show promising 3D
            reconstruction results from the still images based on the geometric parsing.
        </summary>
        <author>
            <name>Zhanglin Peng</name>
        </author>
        <author>
            <name>Ruimao Zhang</name>
        </author>
        <author>
            <name>Xiaodan Liang</name>
        </author>
        <author>
            <name>Xiaobai Liu</name>
        </author>
        <author>
            <name>Liang Lin</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at IJCAI'16</arxiv:comment>
        <link href="http://arxiv.org/abs/1604.01931v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1604.01931v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1604.06529v2</id>
        <updated>2016-06-30T04:23:07Z</updated>
        <published>2016-04-22T03:20:24Z</published>
        <title>Dependency Parsing with LSTMs: An Empirical Evaluation</title>
        <summary>  We propose a transition-based dependency parser using Recurrent Neural
            Networks with Long Short-Term Memory (LSTM) units. This extends the feedforward
            neural network parser of Chen and Manning (2014) and enables modelling of
            entire sequences of shift/reduce transition decisions. On the Google Web
            Treebank, our LSTM parser is competitive with the best feedforward parser on
            overall accuracy and notably achieves more than 3% improvement for long-range
            dependencies, which has proved difficult for previous transition-based parsers
            due to error propagation and limited context information. Our findings
            additionally suggest that dropout regularisation on the embedding layer is
            crucial to improve the LSTM's generalisation.
        </summary>
        <author>
            <name>Adhiguna Kuncoro</name>
        </author>
        <author>
            <name>Yuichiro Sawai</name>
        </author>
        <author>
            <name>Kevin Duh</name>
        </author>
        <author>
            <name>Yuji Matsumoto</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
        <link href="http://arxiv.org/abs/1604.06529v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1604.06529v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1605.08900v2</id>
        <updated>2016-09-24T06:04:15Z</updated>
        <published>2016-05-28T14:47:49Z</published>
        <title>Aspect Level Sentiment Classification with Deep Memory Network</title>
        <summary>  We introduce a deep memory network for aspect level sentiment classification.
            Unlike feature-based SVM and sequential neural models such as LSTM, this
            approach explicitly captures the importance of each context word when inferring
            the sentiment polarity of an aspect. Such importance degree and text
            representation are calculated with multiple computational layers, each of which
            is a neural attention model over an external memory. Experiments on laptop and
            restaurant datasets demonstrate that our approach performs comparable to
            state-of-art feature based SVM system, and substantially better than LSTM and
            attention-based LSTM architectures. On both datasets we show that multiple
            computational layers could improve the performance. Moreover, our approach is
            also fast. The deep memory network with 9 layers is 15 times faster than LSTM
            with a CPU implementation.
        </summary>
        <author>
            <name>Duyu Tang</name>
        </author>
        <author>
            <name>Bing Qin</name>
        </author>
        <author>
            <name>Ting Liu</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published in EMNLP 2016</arxiv:comment>
        <link href="http://arxiv.org/abs/1605.08900v2" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1605.08900v2" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1607.08381v1</id>
        <updated>2016-07-28T09:43:52Z</updated>
        <published>2016-07-28T09:43:52Z</published>
        <title>A Siamese Long Short-Term Memory Architecture for Human
            Re-Identification</title>
        <summary>  Matching pedestrians across multiple camera views known as human
            re-identification (re-identification) is a challenging problem in visual
            surveillance. In the existing works concentrating on feature extraction,
            representations are formed locally and independent of other regions. We present
            a novel siamese Long Short-Term Memory (LSTM) architecture that can process
            image regions sequentially and enhance the discriminative capability of local
            feature representation by leveraging contextual information. The feedback
            connections and internal gating mechanism of the LSTM cells enable our model to
            memorize the spatial dependencies and selectively propagate relevant contextual
            information through the network. We demonstrate improved performance compared
            to the baseline algorithm with no LSTM units and promising results compared to
            state-of-the-art methods on Market-1501, CUHK03 and VIPeR datasets.
            Visualization of the internal mechanism of LSTM cells shows meaningful patterns
            can be learned by our method.
        </summary>
        <author>
            <name>Rahul Rama Varior</name>
        </author>
        <author>
            <name>Bing Shuai</name>
        </author>
        <author>
            <name>Jiwen Lu</name>
        </author>
        <author>
            <name>Dong Xu</name>
        </author>
        <author>
            <name>Gang Wang</name>
        </author>
        <link href="http://arxiv.org/abs/1607.08381v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1607.08381v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1608.06154v1</id>
        <updated>2016-08-22T12:59:31Z</updated>
        <published>2016-08-22T12:59:31Z</published>
        <title>Multi-Sensor Prognostics using an Unsupervised Health Index based on
            LSTM Encoder-Decoder</title>
        <summary>  Many approaches for estimation of Remaining Useful Life (RUL) of a machine,
            using its operational sensor data, make assumptions about how a system degrades
            or a fault evolves, e.g., exponential degradation. However, in many domains
            degradation may not follow a pattern. We propose a Long Short Term Memory based
            Encoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI)
            for a system using multi-sensor time-series data. LSTM-ED is trained to
            reconstruct the time-series corresponding to healthy state of a system. The
            reconstruction error is used to compute HI which is then used for RUL
            estimation. We evaluate our approach on publicly available Turbofan Engine and
            Milling Machine datasets. We also present results on a real-world industry
            dataset from a pulverizer mill where we find significant correlation between
            LSTM-ED based HI and maintenance costs.
        </summary>
        <author>
            <name>Pankaj Malhotra</name>
        </author>
        <author>
            <name>Vishnu TV</name>
        </author>
        <author>
            <name>Anusha Ramakrishnan</name>
        </author>
        <author>
            <name>Gaurangi Anand</name>
        </author>
        <author>
            <name>Lovekesh Vig</name>
        </author>
        <author>
            <name>Puneet Agarwal</name>
        </author>
        <author>
            <name>Gautam Shroff</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st ACM SIGKDD Workshop on Machine Learning for
            Prognostics and Health Management, San Francisco, CA, USA, 2016. 10 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1608.06154v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1608.06154v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1610.05858v1</id>
        <updated>2016-10-19T03:59:30Z</updated>
        <published>2016-10-19T03:59:30Z</published>
        <title>Bidirectional LSTM-CRF for Clinical Concept Extraction</title>
        <summary>  Extraction of concepts present in patient clinical records is an essential
            step in clinical research. The 2010 i2b2/VA Workshop on Natural Language
            Processing Challenges for clinical records presented concept extraction (CE)
            task, with aim to identify concepts (such as treatments, tests, problems) and
            classify them into predefined categories. State-of-the-art CE approaches
            heavily rely on hand crafted features and domain specific resources which are
            hard to collect and tune. For this reason, this paper employs bidirectional
            LSTM with CRF decoding initialized with general purpose off-the-shelf word
            embeddings for CE. The experimental results achieved on 2010 i2b2/VA reference
            standard corpora using bidirectional LSTM CRF ranks closely with top ranked
            systems.
        </summary>
        <author>
            <name>Raghavendra Chalapathy</name>
        </author>
        <author>
            <name>Ehsan Zare Borzeshi</name>
        </author>
        <author>
            <name>Massimo Piccardi</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper "Bidirectional LSTM-CRF for Clinical Concept Extraction"
            is accepted for short paper presentation at Clinical Natural Language
            Processing Workshop at COLING 2016 Osaka, Japan. December 11, 2016</arxiv:comment>
        <link href="http://arxiv.org/abs/1610.05858v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1610.05858v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1610.08936v3</id>
        <updated>2017-10-05T01:14:56Z</updated>
        <published>2016-10-27T19:08:57Z</published>
        <title>Learning Scalable Deep Kernels with Recurrent Structure</title>
        <summary>  Many applications in speech, robotics, finance, and biology deal with
            sequential data, where ordering matters and recurrent structures are common.
            However, this structure cannot be easily captured by standard kernel functions.
            To model such structure, we propose expressive closed-form kernel functions for
            Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the
            inductive biases of long short-term memory (LSTM) recurrent networks, while
            retaining the non-parametric probabilistic advantages of Gaussian processes. We
            learn the properties of the proposed kernels by optimizing the Gaussian process
            marginal likelihood using a new provably convergent semi-stochastic gradient
            procedure and exploit the structure of these kernels for scalable training and
            prediction. This approach provides a practical representation for Bayesian
            LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and
            thoroughly investigate a consequential autonomous driving application, where
            the predictive uncertainties provided by GP-LSTM are uniquely valuable.
        </summary>
        <author>
            <name>Maruan Al-Shedivat</name>
        </author>
        <author>
            <name>Andrew Gordon Wilson</name>
        </author>
        <author>
            <name>Yunus Saatchi</name>
        </author>
        <author>
            <name>Zhiting Hu</name>
        </author>
        <author>
            <name>Eric P. Xing</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 7 figures, 5 tables. Updated to the final version that
            appears in JMLR, 18(82):1-37, 2017</arxiv:comment>
        <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Machine Learning Research (JMLR), JMLR 18(82):1-37,
            2017</arxiv:journal_ref>
        <link href="http://arxiv.org/abs/1610.08936v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1610.08936v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
        <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1611.05125v3</id>
        <updated>2017-05-18T05:55:24Z</updated>
        <published>2016-11-16T02:56:24Z</published>
        <title>Learning To Score Olympic Events</title>
        <summary>  Estimating action quality, the process of assigning a "score" to the
            execution of an action, is crucial in areas such as sports and health care.
            Unlike action recognition, which has millions of examples to learn from, the
            action quality datasets that are currently available are small -- typically
            comprised of only a few hundred samples. This work presents three frameworks
            for evaluating Olympic sports which utilize spatiotemporal features learned
            using 3D convolutional neural networks (C3D) and perform score regression with
            i) SVR, ii) LSTM, and iii) LSTM followed by SVR. An efficient training
            mechanism for the limited data scenarios is presented for clip-based training
            with LSTM. The proposed systems show significant improvement over existing
            quality assessment approaches on the task of predicting scores of Olympic
            events {diving, vault, figure skating}. While the SVR-based frameworks yield
            better results, LSTM-based frameworks are more natural for describing an action
            and can be used for improvement feedback.
        </summary>
        <author>
            <name>Paritosh Parmar</name>
        </author>
        <author>
            <name>Brendan Tran Morris</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2017 - CVSports Workshop</arxiv:comment>
        <link href="http://arxiv.org/abs/1611.05125v3" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1611.05125v3" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1611.05588v1</id>
        <updated>2016-11-17T06:57:01Z</updated>
        <published>2016-11-17T06:57:01Z</published>
        <title>Instance-aware Image and Sentence Matching with Selective Multimodal
            LSTM</title>
        <summary>  Effective image and sentence matching depends on how to well measure their
            global visual-semantic similarity. Based on the observation that such a global
            similarity arises from a complex aggregation of multiple local similarities
            between pairwise instances of image (objects) and sentence (words), we propose
            a selective multimodal Long Short-Term Memory network (sm-LSTM) for
            instance-aware image and sentence matching. The sm-LSTM includes a multimodal
            context-modulated attention scheme at each timestep that can selectively attend
            to a pair of instances of image and sentence, by predicting pairwise
            instance-aware saliency maps for image and sentence. For selected pairwise
            instances, their representations are obtained based on the predicted saliency
            maps, and then compared to measure their local similarity. By similarly
            measuring multiple local similarities within a few timesteps, the sm-LSTM
            sequentially aggregates them with hidden states to obtain a final matching
            score as the desired global similarity. Extensive experiments show that our
            model can well match image and sentence with complex content, and achieve the
            state-of-the-art results on two public benchmark datasets.
        </summary>
        <author>
            <name>Yan Huang</name>
        </author>
        <author>
            <name>Wei Wang</name>
        </author>
        <author>
            <name>Liang Wang</name>
        </author>
        <link href="http://arxiv.org/abs/1611.05588v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1611.05588v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1612.01022v1</id>
        <updated>2016-12-03T21:30:26Z</updated>
        <published>2016-12-03T21:30:26Z</published>
        <title>Short-term traffic flow forecasting with spatial-temporal correlation in
            a hybrid deep learning framework</title>
        <summary>  Deep learning approaches have reached a celebrity status in artificial
            intelligence field, its success have mostly relied on Convolutional Networks
            (CNN) and Recurrent Networks. By exploiting fundamental spatial properties of
            images and videos, the CNN always achieves dominant performance on visual
            tasks. And the Recurrent Networks (RNN) especially long short-term memory
            methods (LSTM) can successfully characterize the temporal correlation, thus
            exhibits superior capability for time series tasks. Traffic flow data have
            plentiful characteristics on both time and space domain. However, applications
            of CNN and LSTM approaches on traffic flow are limited. In this paper, we
            propose a novel deep architecture combined CNN and LSTM to forecast future
            traffic flow (CLTFP). An 1-dimension CNN is exploited to capture spatial
            features of traffic flow, and two LSTMs are utilized to mine the short-term
            variability and periodicities of traffic flow. Given those meaningful features,
            the feature-level fusion is performed to achieve short-term forecasting. The
            proposed CLTFP is compared with other popular forecasting methods on an open
            datasets. Experimental results indicate that the CLTFP has considerable
            advantages in traffic flow forecasting. in additional, the proposed CLTFP is
            analyzed from the view of Granger Causality, and several interesting properties
            of CLTFP are discovered and discussed .
        </summary>
        <author>
            <name>Yuankai Wu</name>
        </author>
        <author>
            <name>Huachun Tan</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
        <link href="http://arxiv.org/abs/1612.01022v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1612.01022v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
    <entry>
        <id>http://arxiv.org/abs/1701.05847v1</id>
        <updated>2017-01-20T16:36:09Z</updated>
        <published>2017-01-20T16:36:09Z</published>
        <title>End-To-End Visual Speech Recognition With LSTMs</title>
        <summary>  Traditional visual speech recognition systems consist of two stages, feature
            extraction and classification. Recently, several deep learning approaches have
            been presented which automatically extract features from the mouth images and
            aim to replace the feature extraction stage. However, research on joint
            learning of features and classification is very limited. In this work, we
            present an end-to-end visual speech recognition system based on Long-Short
            Memory (LSTM) networks. To the best of our knowledge, this is the first model
            which simultaneously learns to extract features directly from the pixels and
            perform classification and also achieves state-of-the-art performance in visual
            speech classification. The model consists of two streams which extract features
            directly from the mouth and difference images, respectively. The temporal
            dynamics in each stream are modelled by an LSTM and the fusion of the two
            streams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement
            of 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the
            CUAVE database when compared with other methods which use a similar visual
            front-end.
        </summary>
        <author>
            <name>Stavros Petridis</name>
        </author>
        <author>
            <name>Zuwei Li</name>
        </author>
        <author>
            <name>Maja Pantic</name>
        </author>
        <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication, ICASSP 2017</arxiv:comment>
        <link href="http://arxiv.org/abs/1701.05847v1" rel="alternate" type="text/html"/>
        <link title="pdf" href="http://arxiv.org/pdf/1701.05847v1" rel="related" type="application/pdf"/>
        <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
        <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    </entry>
</feed>